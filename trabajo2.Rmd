---
title: "**UNIVERSIDAD NACIONAL AGRARIA LA MOLINA**"
output:
  pdf_document:
    latex_engine: xelatex
    highlight: tango
    toc: false
header-includes:
- \usepackage{graphicx}
editor_options: 
  markdown: 
    wrap: 72
---

\pagenumbering{gobble}
\begin{center}
\vspace{0.1cm}
{\large DEPARTAMENTO DE ESTADÍSTICA E INFORMÁTICA}

\vspace{0.1cm}
{\large FACULTAD DE ECONOMÍA Y PLANIFICACIÓN}

\vspace{1cm}
\includegraphics[width=0.25\textwidth]{logo_unalm.png}

\vspace{1.5cm}
{\Large \textbf{TRABAJO INTEGRADOR}}

\vspace{0.5cm}
{\large \textbf{Inferencia Estadística 2025-II}}

\vspace{1.5cm}
\begin{tabular}{|l|r|}
\hline
\textbf{Integrante} & \textbf{Código} \\
\hline
Castillo Ruiz Mauricio Gabriel & 20230384 \\
\hline
Gómez Vigo Héctor Estefano & 20230397 \\
\hline
Montúfar Paiva Yeraldi Mercedes & 20230400 \\
\hline
Paucar Arango Marcos David Alexander & 20221412 \\
\hline
Rojas Taco Fabiana & 20220956 \\
\hline
Villanueva Huamani Alexander Ruben & 20230419 \\
\hline
Zavala Malpartida Kay Daniela L. & 20230420 \\
\hline
\end{tabular}

\vspace{1.3cm}
\textbf{Docente:} Fernando Miranda Villagómez

\vspace{1cm}
\textbf{LA MOLINA - LIMA - PERÚ 2025}

\end{center}
\newpage
\pagenumbering{arabic}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}

# 1. UNA SOLA BASE DE DATOS ORIGINAL (n = 1000)
set.seed(123)
alpha_true <- 2
beta_true <- 3
n <- 1000
reclamos <- alpha_true / (1 - runif(n))^(1/beta_true)
```

## Distribución Pareto:

$$
\begin{aligned}
&f(x)=\frac{\beta \alpha^\beta}{x^{\beta+1}} I_{(\alpha, \infty)}(x)\\
&E(X) = \frac{\alpha \beta}{\beta - 1} \quad \text{; } \quad \operatorname{VAR}(X) = \left( \frac{\alpha}{\beta - 1} \right)^2 \frac{\beta}{\beta - 2}
\end{aligned}
$$

# Pregunta 1

## 1.1 EMV para parametro alfa:

Usaremos el Método de máxima verosimilitud para poder hallar el primer
estimador para alfa:

$$
\begin{aligned}
L(\alpha, \beta) 
&= \prod_{i=1}^n f(x_i ; \alpha, \beta) 
= \prod_{i=1}^n \frac{\beta \alpha^\beta}{x_i^{\beta+1}} \, I_{[\alpha, \infty)}(x_i)\ 
&= \beta^n \alpha^{n\beta} \left( \prod_{i=1}^n x_i^{-(\beta+1)} \right) 
\times \prod_{i=1}^n I_{[\alpha, \infty)}(x_i)
\end{aligned}
$$ Analizando la indicadora:

$$
\begin{aligned}
\prod_{i=1}^n I_{[\alpha, \infty]}\left(x_i\right) & =I\left(\bigcap_{i=i}^n\left\{x_i \geq \alpha\right\}\right) \\
& =I\left(\alpha \leq x_1, \alpha \leq x_2, \ldots \alpha \leq x_n\right) \\
& =I\left(\alpha \leq m i\left(x_1, \ldots, x_n\right\}\right)=I\left(\alpha \leq y_1\right]
\end{aligned}
$$ donde y1 = X(1) es el mínimo de la muestra:

$$
El \text{ EMV de } \theta = \alpha \text{ es } \; T = Y_{1}.
$$

### 1.1.2 Propiedades:

#### a) Insesgamiento:

Distribución del mínimo:

$$
\begin{aligned}
&X_1 \ldots X_n \sim \text { Pareto }(\alpha, \beta) \text { i i d }\\
&\begin{aligned}
& \quad F(x)=1-\left(\frac{\alpha}{x}\right)^\beta, x \geqslant \alpha \\
& y=x_i \\
& F x_i(y)=n[1-F(y)]^{n-1} \mathcal{F}(y) \rightarrow n\left[\left(\frac{\alpha}{\beta}\right)^\beta\right]^{\beta-1} \cdot \frac{\beta \alpha^\beta}{y^{\beta+1}} \\
& =n\left[\frac{\alpha^{\beta(n-1)}}{\alpha^{\beta(n-1)}}\right] \cdot \frac{\beta \alpha^\beta}{y^{\beta+1}}=\frac{(n \beta) \alpha^{n \beta}}{y^{n \beta+1}}, y \geqslant \alpha \\
& \\
& \quad \rightarrow x_1 \sim \operatorname{Pareto}(\alpha, n \beta)
\end{aligned}
\end{aligned}
$$

Hay sesgo positivo, es decir que sobrestima al parametro, no cumple
propiedades de Insesgamiento. Ahora lo veremos mejor con los valores
para nuestra data:

```{r}
# 1. Valor observado del mínimo 
alpha_EMV_observado <- min(reclamos)
# 2. Valor esperado teórico del mínimo
E_alpha_EMV_teorico <- (n * beta_true * alpha_true) / (n * beta_true - 1)
```

```{r, echo=FALSE}
print(paste("El valor observado es:", alpha_EMV_observado, "y el valor esperado es:", E_alpha_EMV_teorico))
```

El valor observado en los datos fue de 2.00031, mientras que el valor
esperado teórico es 2.00067, ambos ligeramente por encima del valor real
del parámetro (alfa = 2.00000). Esto confirma que, para muestras
finitas, el estimador tiende a sobrestimar el parámetro verdadero,
aunque en este caso la magnitud del sesgo es muy pequeña (del orden de
0.0003 a 0.0007). La cercanía entre el valor observado y el teórico
valida la expresión matemática del sesgo y respalda la propiedad de
insesgamiento asintótico, ya que a medida que el tamaño de muestra
aumenta, el sesgo tiende a cero.

```{r, echo=FALSE, warning=FALSE}
# =============================================================================
# DEMOSTRACIÓN VISUAL: SESGO POSITivo Y CONVERGENCIA ASINTÓTICA
# =============================================================================
# 1. UNA SOLA BASE DE DATOS ORIGINAL (n = 1000)
set.seed(123)
alpha_true <- 2
beta_true <- 3
n_total <- 1000
reclamos <- alpha_true / (1 - runif(n_total))^(1/beta_true)

# 2. Tamaños de submuestra 
n_values <- seq(10, 1000, by = 10)

# 3. Calcular E[α_EMV] teórico para cada n
results <- data.frame(n = n_values)
results$E_teorico <- (results$n * beta_true * alpha_true) / (results$n * beta_true - 1)

# 4. Gráfico de convergencia
library(ggplot2)

ggplot(results, aes(x = n)) +
  geom_line(aes(y = E_teorico, color = "E[alfa_EMV] teórico"), linewidth = 1.5) +
  geom_hline(yintercept = alpha_true, color = "red", linetype = "dashed", linewidth = 1) +
  geom_ribbon(aes(ymin = alpha_true, ymax = E_teorico), fill = "pink", alpha = 0.3) +
  labs(
    title = "Sesgo Positivo y Convergencia Asintótica de alfa_EMV",
    x = "Tamaño de muestra (n)",
    y = "Valor esperado E[alfa_EMV]",
    color = ""
  ) +
  scale_color_manual(values = c("E[alfa_EMV] teórico" = "blue")) +
  theme_minimal() +
  annotate("text", x = 500, y = 2.15, 
           label = "Sesgo positivo: E[alfa_EMV] > alfa", 
           color = "darkred", size = 4.5, fontface = "bold") +
  annotate("text", x = 800, y = 2.02, 
           label = paste("Límite asintótico: α =", alpha_true), 
           color = "red", size = 4) +
  ylim(1.95, 2.2)
```

de nuestra base de datos, tomamos submuestras que van incrementando su
valor para verificar que tendiendo al infinito (tamaños suficientemente
grande) podemos ver como el valor tiende al parámetro de alfa = 2.

#### b) Consistencia:

Podemos hacer uso del Teorema 2, por la propiedad anterior del
insesgamiento asintotico

$$
\begin{aligned}
& \bullet \quad \lim_{n \rightarrow \infty} E(y_1) = \alpha \quad \text{ya que} \quad 
\lim_{n \rightarrow \infty} \frac{n \beta \cdot \alpha}{n \beta - 1} 
= \lim_{n \rightarrow \infty} \frac{\beta \alpha}{\beta - \frac{1}{n}} = \frac{\beta \alpha}{\beta} = \alpha \\
& \bullet \quad \lim_{n \rightarrow \infty} \operatorname{Var}(y_1) 
= \lim_{n \rightarrow \infty} \frac{n \beta \alpha^2}{(n \beta - 1)^2 (n \beta - 2)} 
= \lim_{n \rightarrow \infty} \frac{n \beta \alpha^2}{n^3 \beta^3} 
= \lim_{n \rightarrow \infty} \frac{\alpha^2}{n^2 \beta^2} = 0
\end{aligned}
$$

Como podemos obervar cumple ambas condiciones, por ende decimos que
nuestro estimador es consistente, en otras palabras el estimador converge en probabilidades al parametro.

```{r, include=FALSE}
## Aca vemos la var de la muestra total, para ver que es cercana a 0
var_teorica <- (n * beta_true * alpha_true^2) / ((n * beta_true - 1)^2 * (n * beta_true - 2))
var_teorica
```

\*\* Gráfico:\*\* Hemos visto ya un gráfico para solidar la primera
demostración, ahora veamos uno que consolide la segunda condición:

```{r, echo=FALSE}
# =============================================================================
# GRÁFICO: VARIANZA DE α_EMV TIENDE A CERO
# =============================================================================

# Valores para el gráfico
n_var <- seq(10, 1000, by = 10)

# Varianza teórica de α_EMV 
var_teorica <- (n_var * beta_true * alpha_true^2) / ((n_var * beta_true - 1)^2 * (n_var * beta_true - 2))

results_var <- data.frame(n = n_var, varianza = var_teorica)

ggplot(results_var, aes(x = n, y = varianza)) +
  geom_line(color = "purple", linewidth = 1.2) +
  labs(
    title = "Convergencia de la Varianza de alfa_EMV hacia Cero",
    subtitle = "Var(alfa_EMV) tiende 0 cuando n tiende inf",
    x = "Tamaño de muestra n",
    y = "Var(alfa_EMV)"
  ) +
  theme_minimal() +
  annotate("text", x = 600, y = max(var_teorica)/2, 
           label = "Var(alfa_EMV) tiende 0", 
           color = "purple", size = 5) +
  ylim(0, max(var_teorica))
```

#### c) Suficiencia:

$$
\begin{aligned}
& P(x=x / t=T)=\frac{P\left(x_1=x_1 \ldots x_n=x_n, T=y_1\right)}{P(T=y_1) }
\end{aligned}
$$

$$
\begin{aligned}
& \frac{\left(\frac{\beta \alpha^\beta}{x_1^{\beta+1}}\right)\left(\frac{\beta \alpha^\beta}{x_2^{\beta+1}}\right) \cdots\left(\frac{\beta \alpha^\beta}{x_n^{\beta+1}}\right)}{\frac{n \beta \alpha^{n \beta}}{y^{n \beta+1}}}, \quad x_i \geqslant \alpha,\ i=1,2,\ldots,n \\
& = \frac{\beta^n \cdot \alpha^{n \beta} \cdot \prod_{i=1}^n x_i^{-(\beta+1)}}{\frac{n \beta \alpha^{n \beta}}{y^{n \beta+1}}} = \frac{\beta^{n-1} \cdot y^{n \beta+1} \cdot \prod_{i=1}^n x_i^{-(\beta+1)}}{n}
\end{aligned}
$$

Dada la dsitribución condicional dado Y = X1, no depende de alfa,
entonces podemos decir que es una estadística suficiente para alpha,
cumple la propiedad.

#### d) Ancilaridad:

La distribución del mínimo $x_{(1)}$ es:

$$
\begin{gathered}
x_{(1)} \sim \mathrm{Pareto}(\alpha, n\beta) \\
f_{x_{(1)}}(t) = \frac{n \beta \alpha^{n \beta}}{t^{n \beta+1}}, \quad t \geqslant \alpha
\end{gathered}
$$ Como vemos su distribución depende de alfa, no cumple con la
propiedad de ancilaridad.

#### e) Completitud:

Calculamos la esperanza de $g(T)$:

$$
E[g(T)] = \int_{\alpha}^{\infty} g(t) \cdot \frac{n\beta \alpha^{n\beta}}{t^{n\beta+1}} \, dt.
$$

Exigimos que $E[g(T)] = 0$ para todo $\alpha > 0$:

$$
\int_{\alpha}^{\infty} \frac{g(t)}{t^{n\beta+1}} \, dt = 0 \quad \forall \alpha > 0.
$$

Derivamos ambos lados respecto a $\alpha$:

$$
\frac{d}{d\alpha} \left[ \int_{\alpha}^{\infty} \frac{g(t)}{t^{n\beta+1}} \, dt \right] = -\frac{g(\alpha)}{\alpha^{n\beta+1}} = 0.
$$

Esto implica que:

$$
g(\alpha) = 0 \quad \forall \alpha > 0.
$$

Como $\alpha$ es cualquier valor positivo, concluimos que $g(t) = 0$
para todo $t$ en el soporte de $T$. Por lo tanto, $T = X_{(1)}$ es una
**estadística completa** para $\alpha$ cuando $\beta$ es conocido.

Por el Teorema de Bahadur, como es suficiente y completo, podemos
afirmar que nuestro estimador es minimal suficiente.

#### f) Optimalidad:

En nuestro caso alfa_EMV no es óptimo porque es sesgado. Pero si existe
un estimador óptimo basado en la estadística suficiente que se podría
obtener mediante corrección del sesgo. (Teorema de Lehman-Scheffé)

## 1.2 EMV para parametro Beta:

Para hallar el estimador de máxima verosimilitud del parámetro $\beta$,
reemplazamos el estimador obtenido previamente para el parámetro
$\alpha$, el cual es $$
\hat{\alpha}_{MLE} = \min\{X_1, X_2, \dots, X_n\} = X_{(1)}.
$$ Como el máximo de la función de verosimilitud es el mismo que el
máximo de su logaritmo, trabajamos con la log-verosimilitud. La función
de verosimilitud está dada por

$$
L(\alpha, \beta)
= \beta^n \cdot \alpha^{n\beta} \cdot \prod_{i=1}^n X_i^{-(\beta+1)}.
$$

Aplicando logaritmo natural,

$$
\ln(L(\alpha, \beta))
= n \ln(\beta) + n\beta \ln(\alpha)
- (\beta+1)\sum_{i=1}^n \ln(X_i).
$$

Derivando respecto de $\beta$,

$$
\begin{aligned}
\frac{d}{d\beta}\ln(L(\alpha, \beta))
&= \frac{n}{\beta} + n \ln \alpha - \sum_{i=1}^n \ln X_i.
\end{aligned}
$$

Igualando a cero,

$$
\frac{n}{\beta} + n \ln \alpha - \sum_{i=1}^n \ln X_i = 0.
$$

Despejando $\beta$, 
$$
\begin{aligned}
\frac{n}{\beta}
&= \sum_{i=1}^n \ln X_i - n \ln \alpha \\
&= \sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right).
\end{aligned}
$$

Por lo tanto,

$$
\hat{\beta}
= \frac{n}{\sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right)}.
$$

Finalmente, reemplazando $\alpha$ por su estimador de máxima
verosimilitud $\hat{\alpha}_{MLE} = X_{(1)}$, se obtiene

$$
\boxed{
\hat{\beta}_{MLE}
= \frac{n}{\sum_{i=1}^n \ln\left(\frac{X_i}{X_{(1)}}\right)}
}.
$$

### 1.2.1 Propiedades:

#### a) Insesgamiento

Primero analizaremos una parte del denominador para determinar su
distribución y se facilite el procedimiento.

$$
\begin{aligned}
F_Y(y) &= P(Y_i \leq y) = P\left( \ln\left( \frac{X_i}{\alpha} \right) \leq y \right) = P(X_i \leq \alpha e^y) \\
&= 1 - \left( \frac{\alpha}{\alpha e^y} \right)^\beta = 1 - e^{-\beta y}, \quad y \geq 0 \\
f_Y(y) &= \frac{d}{dy} F_Y(y) = \beta e^{-\beta y}, \quad y \geq 0
\end{aligned}
$$ Analizando el resultado de la distribución, notamos que tiene la
forma de la exponencial. Entonces el mínimo igual será exponencial con
parametro n por beta. Ademas tomamos a T como la sumatoria, donde por
propiedad de ln de una división podemos desplegarlo.

$$
\begin{aligned}
Y_i &\sim \text{Exponencial}(\beta) \\
W = Y_{(1)} &= \min(Y_1, \dots, Y_n) \sim \text{Exp}(n\beta) \\
T &= \sum_{i=1}^n (Y_i - Y_{(1)}) \sim \text{Gamma}(n-1, \beta) \\
\hat{\beta} &= \frac{n}{T}, \quad T \sim \text{Gamma}(n-1, \beta) \\
E\left[ \frac{1}{T} \right] &= \frac{\beta}{n-2}, \quad n > 2 \\
E[\hat{\beta}] &= n \cdot E\left[ \frac{1}{T} \right] = \frac{n\beta}{n-2}
\end{aligned}
$$ Acá vemos que hay un sesgo presente

$$
\text{Sesgo} = E[\hat{\beta}] - \beta = \frac{n\beta}{n-2} - \beta = \frac{2\beta}{n-2}
$$

Por ende, no es un estimador insesgado. Ademas al ser positivo podemos
decir que sobrestima al parametro.

Ahora lo aplicaremos a nuestra data: 

```{r include=FALSE}
# =============================================================================
# INSesgamiento del EMV de beta - aplicación con la data
# =============================================================================

# tamaño muestral
n <- length(reclamos)

# estimador emv de alfa
alpha_emv_observado <- min(reclamos)

# estimador emv de beta (observado)
beta_emv_observado <- n / sum(log(reclamos / alpha_emv_observado))

# valor esperado teórico del emv de beta
E_beta_emv_teorico <- (n * beta_true) / (n - 2)
```

```{r echo=FALSE}
print(paste("beta_emv observado:", beta_emv_observado))
print(paste("E[beta_emv] teorico:", E_beta_emv_teorico))
print(paste("beta verdadero:", beta_true))

```
El valor observado del estimador fue ligeramente mayor que el valor
verdadero del parámetro $\beta$, mientras que el valor esperado teórico
también se encuentra por encima de $\beta$.

Esto confirma empíricamente que el estimador EMV de $\beta$ presenta
sesgo positivo, es decir, sobrestima el parámetro verdadero para
muestras finitas, por lo que no cumple la propiedad de insesgamiento.

Sin embargo, la diferencia entre el valor observado y el valor verdadero
es pequeña, lo cual sugiere que el sesgo disminuye conforme el tamaño
muestral aumenta.

```{r}
# =============================================================================
# DEMOSTRACIÓN VISUAL: SESGO POSITIVO Y CONVERGENCIA ASINTÓTICA DE beta_EMV
# =============================================================================

set.seed(123)

alpha_true <- 2
beta_true <- 3
n_total <- 1000

# base de datos original
reclamos <- alpha_true / (1 - runif(n_total))^(1 / beta_true)

# tamaños de submuestra
n_values <- seq(10, 1000, by = 10)

# esperanza teorica del emv de beta para cada n
results <- data.frame(n = n_values)
results$E_teorico <- (results$n * beta_true) / (results$n - 2)

library(ggplot2)

ggplot(results, aes(x = n)) +
  geom_line(aes(y = E_teorico, color = "E[beta_EMV] teorico"), linewidth = 1.4) +
  geom_hline(yintercept = beta_true, linetype = "dashed", linewidth = 1) +
  geom_ribbon(aes(ymin = beta_true, ymax = E_teorico), alpha = 0.25) +
  labs(
    title = "Sesgo positivo y convergencia asintotica del EMV de beta",
    x = "tamaño de muestra (n)",
    y = "valor esperado E[beta_EMV]",
    color = ""
  ) +
  theme_minimal() +
  annotate(
    "text",
    x = 300,
    y = max(results$E_teorico),
    label = "sesgo positivo: E[beta_EMV] > beta",
    size = 4,
    hjust = 0
  ) +
  annotate(
    "text",
    x = 700,
    y = beta_true + 0.05,
    label = paste("limite asintotico: beta =", beta_true),
    size = 4,
    hjust = 0
  )

```

De nuestra base de datos se tomaron submuestras de tamaño creciente con
el fin de analizar el comportamiento del estimador EMV de $\beta$.
Se observa que el valor esperado teórico del estimador se mantiene por
encima del parámetro verdadero, confirmando el sesgo positivo.

No obstante, a medida que el tamaño muestral aumenta, el valor esperado
de $\hat\beta$ se aproxima cada vez más a $\beta$, lo que evidencia
la propiedad de insesgamiento asintótico del estimador.

#### b) Consistencia

Recordemos que el estimador de máxima verosimilitud para $\beta$ está
dado por

$$
\hat{\beta} = \frac{n}{\sum_{i=1}^n \ln\left(\frac{X_i}{X_{(1)}}\right)} = \frac{n}{T},
$$

donde

$$
T = \sum_{i=1}^n (Y_i - Y_{(1)}), \quad Y_i = \ln\left(\frac{X_i}{\alpha}\right).
$$

Se sabe que

$$
\frac{T}{n} \xrightarrow{p} E(Y_i) = \frac{1}{\beta},
$$

por la Ley de los Grandes Números. Por el teorema de continuidad,

$$
\hat{\beta}
= \frac{1}{T/n}
\xrightarrow{p} \beta.
$$

Por lo tanto, $\hat{\beta}$ es un estimador consistente del parámetro
$\beta$.

Como se mostró previamente, el estimador EMV de $\beta$ presenta sesgo
positivo para muestras finitas. Además, la varianza del estimador disminuye conforme aumenta el tamaño muestral. Al cumplirse que el sesgo y la varianza tienden a cero, se concluye que $\hat\beta$ es un estimador consistente del parámetro $\beta$.
```{r include=FALSE}
# =============================================================================
# VARIANZA TEÓRICA DEL EMV DE beta
# =============================================================================

n_var <- seq(10, 1000, by = 10)

var_beta_emv <- (n_var^2 * beta_true^2) / ((n_var - 2)^2 * (n_var - 3))

results_var <- data.frame(n = n_var, varianza = var_beta_emv)

```

```{r echo=FALSE}
ggplot(results_var, aes(x = n, y = varianza)) +
  geom_line(linewidth = 1.2) +
  labs(
    title = "Convergencia de la Varianza del EMV de beta hacia Cero",
    subtitle = "Var(beta_EMV) → 0 cuando n → ∞",
    x = "Tamaño de muestra (n)",
    y = "Var(beta_EMV)"
  ) +
  theme_minimal() +
  annotate(
    "text",
    x = 600,
    y = max(var_beta_emv) / 2,
    label = "Var(beta_EMV) tiende a 0",
    size = 5
  )

```
Como se observa, la varianza del estimador EMV de $\beta$ tiende a
cero conforme el tamaño muestral aumenta. Dado que el sesgo del
estimador también tiende a cero, se concluye que el estimador $\hat\beta$ es consistente. En otras palabras, el estimador
converge en probabilidad al verdadero valor del parámetro $\beta$.

#### c) Suficiencia

$$
L(\beta;\mathbf{x})
= \beta^n \exp\left\{-\beta \sum_{i=1}^n
\ln\left(\frac{x_i}{x_{(1)}}\right)\right\}
\cdot \prod_{i=1}^n \frac{1}{x_i}.
$$

Se define el estadístico

$$
T = \sum_{i=1}^n \ln\left(\frac{X_i}{X_{(1)}}\right).
$$

Entonces la verosimilitud puede factorizarse como

$$
L(\beta;\mathbf{x}) =
\underbrace{\beta^n e^{-\beta T}}_{g(T,\beta)}
\cdot
\underbrace{\prod_{i=1}^n \frac{1}{x_i}}_{h(\mathbf{x})}.
$$

Por el teorema de factorización de Neyman--Fisher, el estadístico $T$ es
suficiente para el parámetro $\beta$.

#### d) Anciliaridad

El estadístico de orden mínimo $$
X_{(1)} = \min\{X_1,\dots,X_n\}
$$

tiene distribución

$$
f_{X_{(1)}}(x) = n \beta \frac{\alpha^{n\beta}}{x^{n\beta+1}},
\quad x > \alpha,
$$

la cual no depende del parámetro $\beta$ cuando se considera la variable
transformada $X_{(1)}/\alpha$.

Por lo tanto, $X_{(1)}$ es un estadístico ancilar y aporta información
únicamente sobre $\alpha$, mas no sobre $\beta$.

#### e) Completitud

El estadístico

$$
T = \sum_{i=1}^n (Y_i - Y_{(1)})
$$

tiene distribución Gamma$(n-1,\beta)$, que pertenece a la familia exponencial
de un solo parámetro.

Dado que la familia Gamma con parámetro de forma conocido es completa, se
concluye que el estadístico $T$ es completo para el parámetro $\beta$.

#### f) Optimalidad

El estimador de máxima verosimilitud $\hat{\beta}$ es un estimador sesgado, por lo que no puede ser óptimo en el sentido clásico de mínima varianza entre los estimadores insesgados (UMVU).

No obstante, al ser un estimador de máxima verosimilitud, $\hat{\beta}$ es consistente y asintóticamente eficiente, alcanzando la cota de
Cramér--Rao cuando $n \to \infty$.

Por lo tanto, $\hat{\beta}$ no es óptimo en muestras finitas, pero sí es
óptimo en sentido asintótico.

Un estimador insesgado para $\beta$ es

$$
\tilde{\beta} = \frac{n-2}{n}\hat{\beta},
$$

el cual depende únicamente del estadístico suficiente y completo $T$.
Por el teorema de Lehmann--Scheffé, $\tilde{\beta}$ es el estimador UMVU
del parámetro $\beta$.



## 1.3 Estimador para la Media Poblacional ($\mu$)

Consideramos la media muestral como estimador natural del valor esperado
de la distribución:

$$ \bar{X} = T(X_1, \dots, X_n) = \frac{1}{n} \sum_{i=1}^n X_i $$

Para que los momentos existan en una distribución Pareto, debemos asumir
que $\beta > 1$ (para la esperanza) y $\beta > 2$ (para la varianza). El
parámetro a estimar es:
$$ \mu = E(X) = \frac{\beta \alpha}{\beta - 1} $$

### 1.3.1 Propiedades:

#### a) Insesgamiento:

Calculamos el valor esperado del estimador:

$$
\begin{aligned}
E[\bar{X}] &= E\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \\
&= \frac{1}{n} \sum_{i=1}^n E[X_i] \\
&= \frac{1}{n} \cdot n \cdot \left( \frac{\beta \alpha}{\beta - 1} \right) = \frac{\beta \alpha}{\beta - 1} = \mu
\end{aligned}
$$

El estimador es **estrictamente insesgado** para $\mu$. Verificamos esto
con la base de datos:

```{r}
# 1. Valor observado de la media muestral
mu_muestral_obs <- mean(reclamos)

# 2. Valor esperado teórico (mu)
mu_teorico <- (beta_true * alpha_true) / (beta_true - 1)
```

```{r, echo=FALSE}
print(paste("La media muestral observada es:", round(mu_muestral_obs, 5), 
            "y el valor esperado teórico es:", round(mu_teorico, 5)))
```

#### b) Consistencia:

Para demostrar la consistencia, verificamos las condiciones del Teorema
de Chebyshev (asumiendo $\beta > 2$):

1.  **Insesgadez:** $\lim_{n \rightarrow \infty} E(\bar{X}) = \mu$ (ya
    demostrado).
2.  **Varianza tiende a cero:** $$
    \begin{aligned}
    \operatorname{Var}(\bar{X}) &= \frac{\operatorname{Var}(X)}{n} \\
    &= \frac{1}{n} \left[ \left( \frac{\alpha}{\beta - 1} \right)^2 \frac{\beta}{\beta - 2} \right] \\
    \lim_{n \rightarrow \infty} \operatorname{Var}(\bar{X}) &= \lim_{n \rightarrow \infty} \frac{C}{n} = 0
    \end{aligned}
    $$

Al cumplirse ambas condiciones, $\bar{X}$ es un estimador
**consistente** para $\mu$.

```{r, echo=FALSE}
# Simulación de convergencia de la media
n_seq <- seq(10, 1000, by = 10)
medias_n <- sapply(n_seq, function(nn) mean(reclamos[1:nn]))

df_consistencia <- data.frame(n = n_seq, media = medias_n)

ggplot(df_consistencia, aes(x = n, y = media)) +
  geom_line(color = "darkgreen") +
  geom_hline(yintercept = mu_teorico, linetype = "dashed", color = "red") +
  labs(title = "Consistencia de la Media Muestral",
       x = "Tamaño de muestra (n)", y = "Media Muestral") +
  theme_minimal()
```

#### c) Suficiencia:

Aplicamos el Teorema de Factorización de Fisher-Neyman a la función de
verosimilitud:

$$
L(\alpha, \beta) = \underbrace{\beta^n \alpha^{n\beta} \cdot I_{(\alpha, \infty)}(x_{(1)})}_{g(x_{(1)}, \alpha, \beta)} \cdot \underbrace{\left( \prod_{i=1}^n x_i \right)^{-(\beta+1)}}_{h(x_1, \dots, x_n)}
$$

Como se observa, los estadísticos suficientes conjuntos para
$(\alpha, \beta)$ son el mínimo $X_{(1)}$ y el producto $\prod X_i$ (o
equivalentemente $\sum \ln X_i$). La media muestral
$\bar{X} = \frac{1}{n} \sum X_i$ **no puede factorizarse** de manera que
contenga toda la información de los parámetros.

Por lo tanto, $\bar{X}$ **no es un estadístico suficiente** para los
parámetros de la distribución Pareto.

#### d) Ancilaridad:

Un estadístico es ancilar si su distribución no depende de los
parámetros. La distribución de $\bar{X}$ para una Pareto no tiene una
forma cerrada sencilla (es una suma de variables Pareto), pero su
esperanza $E(\bar{X}) = \frac{\alpha \beta}{\beta - 1}$ y su varianza
dependen directamente de $\alpha$ y $\beta$.

Al depender sus momentos (y por ende su distribución) de los parámetros,
el estimador **no es ancilar**.

#### e) Completitud:

Dado que $\bar{X}$ no es un estadístico suficiente para la familia
Pareto, no se suele analizar su completitud como estimador. Sin embargo,
sabemos que el estadístico suficiente conjunto $(X_{(1)}, \sum \ln X_i)$
es completo, pero la suma aritmética $\sum X_i$ no lo es bajo esta
estructura de familia no exponencial (en el sentido de los parámetros
naturales de la Pareto).

#### f) Optimalidad:

Un estimador es óptimo (UMVUE) si es insesgado y su varianza alcanza la
Cota Inferior de Cramér-Rao (CICR) o si es función de un estadístico
suficiente y completo.

1.  **Eficiencia:** Al no ser función del estadístico suficiente
    $(X_{(1)}, \sum \ln X_i)$, la media muestral pierde información.
2.  **Comparación:** Existe otro estimador para $\mu$, basado en los EMV
    de $\alpha$ y $\beta$
    ($\hat{\mu} = \frac{\hat{\beta} \hat{\alpha}}{\hat{\beta}-1}$), que
    asintóticamente tiene menor varianza que $\bar{X}$.

**Conclusión:** $\bar{X}$ **no es un estimador óptimo** para la media de
una población Pareto, aunque sea fácil de calcular e insesgado.

## 1.4 Método de Momentos para los parámetros de $\alpha$ y $\beta$ de la Distribución Pareto

El método de momentos consiste en igualar los $k$ primeros momentos
poblacionales con los correspondientes momentos muestrales, donde $k$ es
el número de parámetros a estimar. Para la distribución Pareto con dos
parámetros $(\alpha, \beta)$, utilizaremos los dos primeros momentos.

### Definición de la distribución Pareto

Sea $X \sim \text{Pareto}(\alpha, \beta)$ con función de densidad:

$$
f_X(x; \alpha, \beta) = \frac{\beta \alpha^\beta}{x^{\beta+1}}, \quad x \geq \alpha > 0, \ \beta > 0
$$

### P1: Cálculo del primer momento poblacional $E[X]$

#### Planteamiento de la integral

$$
E[X] = \int_{\alpha}^{\infty} x \cdot f_X(x) \, dx 
     = \int_{\alpha}^{\infty} x \cdot \frac{\beta \alpha^\beta}{x^{\beta+1}} \, dx
$$

$$
E[X] = \beta \alpha^\beta \int_{\alpha}^{\infty} x \cdot x^{-(\beta+1)} \, dx
     = \beta \alpha^\beta \int_{\alpha}^{\infty} x^{-\beta} \, dx
$$

Para $\beta > 1$ (condición de convergencia):

$$
\int_{\alpha}^{\infty} x^{-\beta} \, dx = \left[ \frac{x^{1-\beta}}{1-\beta} \right]_{\alpha}^{\infty}
$$

Evaluando los límites:

Límite superior cuando $x \to \infty$: $$
\lim_{x \to \infty} \frac{x^{1-\beta}}{1-\beta} = 0 \quad \text{porque } 1-\beta < 0
$$

Límite inferior cuando $x = \alpha$: $$
\frac{\alpha^{1-\beta}}{1-\beta}
$$

### Resultado del primer momento

$$
E[X] = \beta \alpha^\beta \left( 0 - \frac{\alpha^{1-\beta}}{1-\beta} \right)
     = \beta \alpha^\beta \cdot \frac{\alpha^{1-\beta}}{\beta-1}
     = \frac{\beta \alpha}{\beta-1}
$$

**Primer momento poblacional:** $$
\boxed{E[X] = \frac{\alpha\beta}{\beta-1}}, \quad \beta > 1
$$

### P2:Cálculo del segundo momento poblacional $E[X^2]$

#### Planteamiento de la integral

$$
E[X^2] = \int_{\alpha}^{\infty} x^2 \cdot f_X(x) \, dx 
       = \int_{\alpha}^{\infty} x^2 \cdot \frac{\beta \alpha^\beta}{x^{\beta+1}} \, dx
$$

$$
E[X^2] = \beta \alpha^\beta \int_{\alpha}^{\infty} x^2 \cdot x^{-(\beta+1)} \, dx
       = \beta \alpha^\beta \int_{\alpha}^{\infty} x^{1-\beta} \, dx
$$ Para $\beta > 2$ (condición de convergencia):

$$
\int_{\alpha}^{\infty} x^{1-\beta} \, dx = \left[ \frac{x^{2-\beta}}{2-\beta} \right]_{\alpha}^{\infty}
$$

Evaluando los límites:

Límite superior cuando $x \to \infty$: $$
\lim_{x \to \infty} \frac{x^{2-\beta}}{2-\beta} = 0 \quad \text{porque } 2-\beta < 0
$$

Límite inferior cuando $x = \alpha$: $$
\frac{\alpha^{2-\beta}}{2-\beta}
$$

### Resultado del segundo momento

$$
E[X^2] = \beta \alpha^\beta \left( 0 - \frac{\alpha^{2-\beta}}{2-\beta} \right)
       = \beta \alpha^\beta \cdot \frac{\alpha^{2-\beta}}{\beta-2}
       = \frac{\beta \alpha^2}{\beta-2}
$$

**Segundo momento poblacional:** $$
\boxed{E[X^2] = \frac{\alpha^2\beta}{\beta-2}}, \quad \beta > 2
$$

### P3: Momentos muestrales

Dada una muestra aleatoria
$X_1, X_2, \dots, X_n \stackrel{iid}{\sim} \text{Pareto}(\alpha, \beta)$:

#### Primer momento muestral (media muestral)

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
$$

#### Segundo momento muestral

$$
m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2
$$

#### Varianza muestral (relacionada)

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2
$$

Nota: Existe relación entre $m_2$, $\bar{X}$ y $S^2$: $$
m_2 = S^2 \cdot \frac{n-1}{n} + \bar{X}^2
$$

### P4: Sistema de ecuaciones del método de momentos

Igualamos momentos poblacionales con momentos muestrales:

$$
\begin{cases}
\displaystyle \frac{\alpha\beta}{\beta-1} = \bar{X} & \text{(Ecuación I)} \\[10pt]
\displaystyle \frac{\alpha^2\beta}{\beta-2} = m_2 & \text{(Ecuación II)}
\end{cases}
$$

### Resolución del sistema

#### Despejar $\alpha$ de la Ecuación I

De (I): $$
\frac{\alpha\beta}{\beta-1} = \bar{X} \quad \Rightarrow \quad \alpha\beta = \bar{X}(\beta-1)
$$

Despejando $\alpha$: $$
\boxed{\alpha = \bar{X} \cdot \frac{\beta-1}{\beta}} \quad \text{(Ecuación III)}
$$

#### Sustituir en la Ecuación II

Sustituyendo (III) en (II): $$
\frac{\left(\bar{X} \cdot \frac{\beta-1}{\beta}\right)^2 \beta}{\beta-2} = m_2
$$

#### Simplificación algebraica

$$
\frac{\bar{X}^2 \cdot \frac{(\beta-1)^2}{\beta^2} \cdot \beta}{\beta-2} = m_2
$$

$$
\frac{\bar{X}^2 \cdot \frac{(\beta-1)^2}{\beta}}{\beta-2} = m_2
$$

Reordenando: $$
\frac{(\beta-1)^2}{\beta(\beta-2)} = \frac{m_2}{\bar{X}^2} \quad \text{(Ecuación IV)}
$$

### P5: Definición de R

Sea: $$
R = \frac{m_2}{\bar{X}^2}
$$

La Ecuación IV se convierte en: $$
\frac{(\beta-1)^2}{\beta(\beta-2)} = R
$$

### P6: Resolución de la ecuación en $\beta$

#### Eliminar fracción

Multiplicando en cruz: $$
(\beta-1)^2 = R\beta(\beta-2)
$$

#### Expandir ambos lados

**Lado izquierdo:** $$
(\beta-1)^2 = \beta^2 - 2\beta + 1
$$

**Lado derecho:** $$
R\beta(\beta-2) = R\beta^2 - 2R\beta
$$

#### Igualar y reordenar

$$
\beta^2 - 2\beta + 1 = R\beta^2 - 2R\beta
$$

Llevando todo a un lado: $$
\beta^2 - 2\beta + 1 - R\beta^2 + 2R\beta = 0
$$

Agrupando términos: $$
(1 - R)\beta^2 + 2(R - 1)\beta + 1 = 0
$$

#### Forma estándar de ecuación cuadrática

Multiplicando por -1 para mayor claridad: $$
(R - 1)\beta^2 + 2(1 - R)\beta - 1 = 0
$$

Factor común $(R-1)$: $$
(R-1)(\beta^2 - 2\beta) - 1 = 0
$$

#### Aplicar fórmula general

Para una ecuación $a\beta^2 + b\beta + c = 0$: - $a = 1$ - $b = -2$ -
$c = -\frac{1}{R-1}$

Solución: $$
\beta = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} 
       = \frac{2 \pm \sqrt{4 + \frac{4}{R-1}}}{2}
       = \frac{2 \pm 2\sqrt{1 + \frac{1}{R-1}}}{2}
$$

Simplificando: $$
\beta = 1 \pm \sqrt{1 + \frac{1}{R-1}}
$$

### P7: Selección de la raíz apropiada

#### Consideraciones físicas

1.  $\beta > 0$ (parámetro de forma positivo)
2.  Para la existencia de $E[X]$: $\beta > 1$
3.  Para la existencia de $Var(X)$: $\beta > 2$

#### Análisis de las raíces

-   Raíz 1: $\beta_1 = 1 + \sqrt{1 + \frac{1}{R-1}} > 1$ ✓
-   Raíz 2: $\beta_2 = 1 - \sqrt{1 + \frac{1}{R-1}} < 1$ ✗ (no cumple
    $\beta > 1$)

Por lo tanto, seleccionamos la raíz positiva: $$
\hat{\beta}_{MM} = 1 + \sqrt{1 + \frac{1}{R-1}}
$$

### P8: Relación con el coeficiente de variación

#### Expresión de R en términos de $S^2$

Recordemos que:

$$
m_2 = \frac{1}{n} \sum X_i^2 = S^2 \cdot \frac{n-1}{n} + \bar{X}^2
$$

Entonces:

$$
R = \frac{m_2}{\bar{X}^2} = \frac{S^2}{\bar{X}^2} \cdot \frac{n-1}{n} + 1
$$

#### Aproximación para n grande

Para $n$ suficientemente grande, $\frac{n-1}{n} \approx 1$, entonces:

$$
R \approx \frac{S^2}{\bar{X}^2} + 1 = 1 + \left(\frac{S}{\bar{X}}\right)^2
$$

#### Sustitución en la fórmula

Si $R \approx 1 + \left(\frac{S}{\bar{X}}\right)^2$, entonces: $$
R - 1 \approx \left(\frac{S}{\bar{X}}\right)^2
$$

Y:

$$
\frac{1}{R-1} \approx \left(\frac{\bar{X}}{S}\right)^2
$$

Sustituyendo en la fórmula de $\hat{\beta}_{MM}$: $$
\hat{\beta}_{MM} = 1 + \sqrt{1 + \left(\frac{\bar{X}}{S}\right)^2}
$$

### P9: Obtención del estimador

#### Retomando la Ecuación III

De la Ecuación III:

$$
\alpha = \bar{X} \cdot \frac{\beta-1}{\beta}
$$

#### Sustitución

Sustituyendo $\hat{\beta}_{MM}$:

$$
\hat{\alpha}_{MM} = \bar{X} \cdot \frac{\hat{\beta}_{MM} - 1}{\hat{\beta}_{MM}}
$$

### P10: Resumen final de estimadores

#### Estimador para el parámetro de forma $\beta$:

$$
\boxed{\hat{\beta}_{MM} = 1 + \sqrt{1 + \left(\frac{\bar{X}}{S}\right)^2}}
$$

#### Estimador para el parámetro de escala $\alpha$:

$$
\boxed{\hat{\alpha}_{MM} = \bar{X} \cdot \frac{\hat{\beta}_{MM} - 1}{\hat{\beta}_{MM}}}
$$

donde:\
- $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$ (media muestral)\
- $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2$ (varianza
muestral)\
- $S = \sqrt{S^2}$ (desviación estándar muestral)

## 1.4.1 Propiedades del Estimador $\hat{\alpha}_{MM}$

### a) Insesgamiento

El estimador
$\hat{\alpha}_{MM} = \bar{X} \cdot \frac{\hat{\beta}_{MM}-1}{\hat{\beta}_{MM}}$
es una **función no lineal** de variables aleatorias ($\bar{X}$ y $S$).
Para funciones no lineales $g(\cdot)$, generalmente se cumple que
$E[g(X)] \neq g(E[X])$. En particular:

$$
E\left[\bar{X} \cdot \frac{\hat{\beta}_{MM}-1}{\hat{\beta}_{MM}}\right] \neq E[\bar{X}] \cdot \frac{E[\hat{\beta}_{MM}]-1}{E[\hat{\beta}_{MM}]}
$$

Aunque $\hat{\alpha}_{MM}$ es **asintóticamente insesgado**
($\lim_{n\to\infty} E[\hat{\alpha}_{MM}] = \alpha$), para cualquier $n$
finito existe sesgo sistemático.

**Verificación empírica con nuestros datos:**

```{r demo_no_insesgado, echo=TRUE, fig.height=4.5, fig.width=6}
# Usando nuestra base de datos original 'reclamos'
set.seed(123)
alpha_true <- 2
beta_true <- 3
reclamos <- alpha_true / (1 - runif(n))^(1/beta_true)

# 2. Calcular α̂_MM con TU 'reclamos'
alpha_mm_calc <- function(x) {
  x_bar <- mean(x)
  s <- sd(x)
  beta_hat <- 1 + sqrt(1 + (x_bar/s)^2)
  return(x_bar * (beta_hat - 1) / beta_hat)
}

alpha_hat_reclamos <- alpha_mm_calc(reclamos)

# 2. Bootstrap simple
set.seed(123)
boot_vals <- replicate(300, {
  alpha_mm_calc(sample(reclamos, replace = TRUE))
})

# 3. Gráfico sin abline
hist(boot_vals, col = "lightblue", border = "white",
     main = "Distribución de alpha_MM",
     xlab = "estimador alpha_MM", ylab = "Frecuencia")



```

**Cálculos obtenidos:**

**1. Estimación puntual en la data original:** $\hat{\alpha}_{MM}$
calculado = 2.06082

**2. Análisis bootstrap (300 réplicas):** Promedio de
$\hat{\alpha}_{MM}$ M = 2.069037

**3. Comparación con el valor verdadero:**\
Valor verdadero $\alpha$ = 2.00000\
Diferencia = 2.069037 - 2 = 0.069037

**4. Sesgo relativo:**\
Sesgo absoluto = 0.069037\
Sesgo relativo = (0.069037 / 2) × 100% = 3.45%\
¿E[$\hat{\alpha}_{MM}$] = $\alpha$? → **NO** (existe sesgo positivo de
0.069037)

El estimador $\hat{\alpha}_{MM}$ sobrestima sistemáticamente el
parámetro $\alpha$ en aproximadamente 3.45%.

**Conclusión:** $\hat{\alpha}_{MM}$ **no es un estimador insesgado**
para el parámetro $\alpha$ de la distribución Pareto. Aunque podría ser
aproximadamente insesgado para muestras muy grandes, para $n=1000$
observamos sesgo significativo.

### b) Consistencia

**Definición:** Un estimador es consistente si cuando n tiende al
infinito, se acerca al valor verdader Demostrar que $\hat{\alpha}_{MM}$
es **consistente** para $\alpha$, es decir:

$$
\hat{\alpha}_{MM} \xrightarrow{P} \alpha \quad \text{cuando } n \to \infty
$$

#### Consistencia de $\bar{X}$ y $S^2$

Por la **Ley Débil de los Grandes Números (LGN)** para variables i.i.d.
con primer momento finito:

$$
\bar{X} \xrightarrow{P} \mu
$$

Para la varianza muestral, dado que $E[X^2] < \infty$ (pues
$\beta > 2$), se cumple:

$$
S^2 \xrightarrow{P} \sigma^2
$$

#### Consistencia de $\hat{\beta}_{MM}$

Definimos:

$$
T_n = \frac{\bar{X}^2}{S^2}
$$

Por el **teorema de Slutsky** y continuidad de la función
$(a,b) \mapsto a^2/b$:

$$
T_n \xrightarrow{P} \frac{\mu^2}{\sigma^2}
$$

Calculamos:

$$
\frac{\mu^2}{\sigma^2} = 
\frac{\left( \frac{\beta \alpha}{\beta - 1} \right)^2}{\frac{\beta \alpha^2}{(\beta - 1)^2 (\beta - 2)}} = \beta(\beta - 2)
$$

Luego:

$$
T_n \xrightarrow{P} \beta(\beta - 2)
$$

Ahora, $\hat{\beta}_{MM} = 1 + \sqrt{1 + T_n}$. La función
$h(t) = 1 + \sqrt{1 + t}$ es continua para $t \geq 0$. Por el **teorema
de la aplicación continua**:

$$
\hat{\beta}_{MM} \xrightarrow{P} 1 + \sqrt{1 + \beta(\beta - 2)}
$$

Notando que:

$$
1 + \beta(\beta - 2) = \beta^2 - 2\beta + 1 = (\beta - 1)^2
$$

y como $\beta > 2 > 1$:

$$
\sqrt{1 + \beta(\beta - 2)} = \beta - 1
$$

Por tanto:

$$
\hat{\beta}_{MM} \xrightarrow{P} 1 + (\beta - 1) = \beta
$$

Así, $\hat{\beta}_{MM}$ es consistente para $\beta$.

#### Consistencia de $\hat{\alpha}_{MM}$

Tenemos:

$$
\hat{\alpha}_{MM} = \bar{X} \cdot \frac{\hat{\beta}_{MM} - 1}{\hat{\beta}_{MM}}
$$

Definimos:

$$
U_n = \bar{X}, \quad V_n = \frac{\hat{\beta}_{MM} - 1}{\hat{\beta}_{MM}}
$$

Sabemos:

$$
U_n \xrightarrow{P} \mu = \frac{\beta \alpha}{\beta - 1}, \quad
\hat{\beta}_{MM} \xrightarrow{P} \beta
$$

La función $v(b) = \frac{b-1}{b}$ es continua en $b = \beta > 0$. Por el
**teorema de la aplicación continua**:

$$
V_n \xrightarrow{P} \frac{\beta - 1}{\beta}
$$

Finalmente, por el **teorema de Slutsky** aplicado al producto
$U_n \cdot V_n$:

$$
\hat{\alpha}_{MM} \xrightarrow{P} \mu \cdot \frac{\beta - 1}{\beta} = 
\frac{\beta \alpha}{\beta - 1} \cdot \frac{\beta - 1}{\beta} = \alpha
$$

**Conclusión** Bajo las hipótesis:

-   $X_i \stackrel{iid}{\sim} \text{Pareto}(\alpha, \beta)$ con
    $\beta > 2$
-   Uso de la Ley Débil de los Grandes Números
-   Aplicación del teorema de la aplicación continua
-   Uso del teorema de Slutsky para productos/cocientes

se ha demostrado rigurosamente que:

$$
\boxed{\hat{\alpha}_{MM} \xrightarrow{P} \alpha}
$$

Por lo tanto, $\hat{\alpha}_{MM}$ es un **estimador consistente** del
parámetro de escala $\alpha$ de la distribución Pareto.

Aunque $\hat{\alpha}_{MM}$ **no es insesgado** en general (puede
verificarse por expansión de Taylor o simulaciones), la propiedad de
consistencia garantiza que converge al valor verdadero cuando
$n \to \infty$, lo cual es suficiente para su validez asintótica.

```{r consistencia12, echo=FALSE, fig.height=4.5, fig.width=6}
# ----------------------------------------------------------
# SIMULACIÓN Y GRÁFICA DE CONSISTENCIA PARA alpha_MM (Pareto)
# ----------------------------------------------------------

# Parámetros verdaderos
set.seed(123)
alpha_true <- 2
beta_true <- 3
n_max <- 1000

# Generar muestra completa de Pareto
# Método: inversa de la función de distribución acumulada
# F(x) = 1 - (alpha/x)^beta  =>  x = alpha / (1 - u)^(1/beta)
u <- runif(n_max)
reclamos <- alpha_true / (1 - u)^(1/beta_true)

# Función para calcular alpha_MM dada una muestra
calcular_alpha_MM <- function(x) {
  n <- length(x)
  x_bar <- mean(x)
  if (n > 1) {
    s2 <- var(x)  # var() usa n-1 en denominador
    beta_MM <- 1 + sqrt(1 + (x_bar^2 / s2))
  } else {
    beta_MM <- NA  # No se puede calcular con n=1
  }
  alpha_MM <- x_bar * (beta_MM - 1) / beta_MM
  return(alpha_MM)
}

# Calcular alpha_MM para diferentes tamaños de muestra
tamanos_muestra <- seq(10, n_max, by = 10)
alpha_estimados <- numeric(length(tamanos_muestra))

for (i in seq_along(tamanos_muestra)) {
  n_actual <- tamanos_muestra[i]
  muestra <- reclamos[1:n_actual]
  alpha_estimados[i] <- calcular_alpha_MM(muestra)
}

# Crear data frame para gráfico
datos_grafico <- data.frame(
  n = tamanos_muestra,
  alpha_hat = alpha_estimados
)

# ----------------------------------------------------------
# GRÁFICO DE CONSISTENCIA
# ----------------------------------------------------------
library(ggplot2)

ggplot(datos_grafico, aes(x = n, y = alpha_hat)) +
  # Línea de los valores estimados
  geom_line(color = "steelblue", size = 1) +
  # Puntos de los valores estimados
  geom_point(color = "steelblue", size = 1.5) +
  # Línea horizontal del valor verdadero
  geom_hline(yintercept = alpha_true, 
             color = "red", 
             linetype = "dashed", 
             size = 1,
             alpha = 0.7) +
  # Área de convergencia (+/- 10% del valor verdadero)
  geom_ribbon(aes(ymin = alpha_true * 0.9, 
                  ymax = alpha_true * 1.1),
              fill = "green", 
              alpha = 0.1) +
  # Etiqueta del valor verdadero
  annotate("text", 
           x = max(tamanos_muestra) * 0.9, 
           y = alpha_true + 0.05,
           label = paste("α verdadero =", alpha_true),
           color = "red",
           size = 4) +
  # Etiqueta de la banda de convergencia
  annotate("text",
           x = max(tamanos_muestra) * 0.9,
           y = alpha_true * 1.05,
           label = "Banda ±10%",
           color = "darkgreen",
           size = 3,
           alpha = 0.7) +
  # Configuración de ejes y tema
  labs(
    title = "Consistencia del estimador α̂ (Método de Momentos - Pareto)",
    subtitle = paste("α =", alpha_true, ", β =", beta_true, ", n máximo =", n_max),
    x = "Tamaño de muestra (n)",
    y = expression(hat(alpha)[MM])
  ) +
  scale_x_continuous(breaks = seq(0, n_max, by = 250)) +
  scale_y_continuous(
    limits = c(min(alpha_estimados, alpha_true * 0.8), 
               max(alpha_estimados, alpha_true * 1.2)),
    breaks = seq(2.9, 3.3, by = 0.1)  # Ajusta según tus datos
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12)
  )

# ----------------------------------------------------------
# ANÁLISIS NUMÉRICO ADICIONAL
# ----------------------------------------------------------
cat("\n=== RESUMEN DE CONVERGENCIA ===\n")
cat("Valor verdadero de α:", alpha_true, "\n")
cat("Última estimación (n =", n_max, "):", alpha_estimados[length(alpha_estimados)], "\n")
cat("Error relativo:", 
    abs(alpha_estimados[length(alpha_estimados)] - alpha_true) / alpha_true * 100, "%\n")

cat("\nEvolución del error con n:\n")
n_verificar <- c(50, 100, 250, 500, 750, 1000)
for (nv in n_verificar) {
  idx <- which(tamanos_muestra == nv)
  if (length(idx) > 0) {
    error <- abs(alpha_estimados[idx] - alpha_true)
    cat(sprintf("n = %4d: α̂ = %.4f, error = %.4f (%.2f%%)\n", 
                nv, alpha_estimados[idx], error, error/alpha_true*100))
  }
}
```

### c) Eficiencia

El estimador $\hat{\theta}$ no es **eficiente** para $\theta$ porque:

$\hat{\alpha}_{MM}$ falla en ambas:\
- **No es insesgado** = Ya no puede ser eficiente\
- **Su varianza es mayor** que el mínimo teórico

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
set.seed(123)

# Datos de comparación
n <- c(30, 50, 100, 200, 500)
LICR <- 2^2/(n*3^2)  # alpha²/(n*beta²)
var_estimada <- LICR * 5.5  # alpha_MM tiene ~5.5 veces más varianza

df <- data.frame(
  n = n,
  LICR = LICR,
  Var_alpha_MM = var_estimada
)

ggplot(df, aes(x = n)) +
  geom_line(aes(y = LICR, color = "Límite mínimo (LICR)"), size = 1.5) +
  geom_line(aes(y = Var_alpha_MM, color = "Varianza de α̂_MM"), size = 1.5) +
  geom_ribbon(aes(ymin = LICR, ymax = Var_alpha_MM), 
              fill = "red", alpha = 0.1) +
  labs(
    title = "α̂_MM NO es eficiente",
    subtitle = "Su varianza es mucho mayor que el límite mínimo teórico",
    x = "Tamaño de muestra (n)",
    y = "Varianza",
    color = ""
  ) +
  scale_color_manual(values = c("Límite mínimo (LICR)" = "darkgreen", 
                                "Varianza de α̂_MM" = "red")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  annotate("text", x = 300, y = mean(c(LICR[3], var_estimada[3])),
           label = "Área de ineficiencia", color = "red", size = 4)

```

### d) Suficiencia

Para verificar esta propiedad para el estimador se va a utilizar el
teorema de Criterio de Factorización de Neyman-Fisher

**Teorema (Neyman-Fisher):** Una estadística $T(X)$ es suficiente para
$\theta$ si y solo si la función de verosimilitud puede factorizarse
como:

$$
L(\theta; x) = g(T(x); \theta) \cdot h(x)
$$

donde $g$ depende de los datos solo a través de $T(x)$, y $h$ no depende
de $\theta$.

La función de verosimilitud para la muestra es:

$$
\begin{aligned}
L(\alpha, \beta; x) &= \prod_{i=1}^n \frac{\beta \alpha^\beta}{x_i^{\beta+1}} \mathbf{1}_{[\alpha, \infty)}(x_i) \\
&= \beta^n \alpha^{n\beta} \left( \prod_{i=1}^n x_i \right)^{-\beta-1} \cdot \mathbf{1}_{\{x_{(1)} \geq \alpha\}}
\end{aligned}
$$

donde $x_{(1)} = \min\{x_1, \dots, x_n\}$.

Aplicando logaritmo:

$$
\ell(\alpha, \beta; x) = n\log\beta + n\beta\log\alpha - (\beta+1)\sum_{i=1}^n \log x_i + \log\mathbf{1}_{\{x_{(1)} \geq \alpha\}}
$$

Por el criterio de factorización, vemos que:

$$
T(X) = \left( x_{(1)}, \sum_{i=1}^n \log x_i \right)
$$

es **suficiente mínima** para $(\alpha, \beta)$.

**¿Es** $\hat{\alpha}_{MM}$ suficiente?

**Observación clave:** El estimador $\hat{\alpha}_{MM}$ se expresa como:

$$
\hat{\alpha}_{MM} = \bar{X} \cdot \frac{\hat{\beta}_{MM} - 1}{\hat{\beta}_{MM}}
$$

donde $\bar{X}$ y $\hat{\beta}_{MM}$ dependen únicamente de:\
1. $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$\
2. $S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2$

Estas estadísticas **no incluyen** información sobre $x_{(1)}$, que es
componente esencial de la estadística suficiente mínima.

**Conclusión**

Dado que:\
1. La estadística suficiente mínima para $(\alpha, \beta)$ es
$T(X) = \left( x_{(1)}, \sum \log x_i \right)$\
2. $\hat{\alpha}_{MM}$ no depende de $x_{(1)}$\
3. No existe función $h$ tal que $\hat{\alpha}_{MM} = h(T(X))$
preservando toda la información sobre $\alpha$

Se concluye que:

$$
\boxed{\hat{\alpha}_{MM} \text{ NO es un estimador suficiente para } \alpha}
$$

**Implicación**

Al no ser suficiente, $\hat{\alpha}_{MM}$ **podría mejorarse** aplicando
el teorema de Rao-Blackwell, condicionando sobre la estadística
suficiente $T(X)$. El estimador mejorado sería:

$$
\hat{\alpha}_{RB} = E\left[ \hat{\alpha}_{MM} \mid x_{(1)}, \sum \log x_i \right]
$$

el cual tendría menor o igual varianza que $\hat{\alpha}_{MM}$.

### e) Completitud

Una estadística $T(X)$ es **completa** para $\alpha$ si:

$$
E_\alpha[g(T(X))] = 0 \ \forall \alpha > 0 \ \Rightarrow \ P_\alpha(g(T(X)) = 0) = 1 \ \forall \alpha > 0
$$

Es decir: si una función de $T$ tiene esperanza cero para todo $\alpha$,
entonces esa función es casi seguramente cero.

### Estadística completa para $\alpha$ (con $\beta$ conocido)

La densidad se simplifica: $$
f(x; \alpha) = \frac{\beta \alpha^\beta}{x^{\beta+1}} \mathbf{1}_{[\alpha, \infty)}(x), \quad \beta \text{ conocido}
$$

La verosimilitud es: $$
L(\alpha; x) = \beta^n \alpha^{n\beta} \left( \prod_{i=1}^n x_i \right)^{-\beta-1} \cdot \mathbf{1}_{\{x_{(1)} \geq \alpha\}}
$$

Por el **criterio de factorización**: $$
T(X) = X_{(1)} \quad \text{es suficiente para } \alpha
$$

Además, $X_{(1)}$ tiene densidad: $$
f_{X_{(1)}}(t) = \frac{n\beta}{\alpha^{n\beta}} t^{n\beta-1} \mathbf{1}_{[\alpha, \infty)}(t), \quad t > 0
$$

Para verificar completitud, sea $g$ tal que: $$
E_\alpha[g(X_{(1)})] = \int_\alpha^\infty g(t) \cdot \frac{n\beta}{\alpha^{n\beta}} t^{n\beta-1} dt = 0 \ \forall \alpha > 0
$$

Derivando respecto a $\alpha$ (bajo condiciones de regularidad): $$
\frac{d}{d\alpha} E_\alpha[g(X_{(1)})] = -g(\alpha) \cdot \frac{n\beta}{\alpha} = 0 \ \forall \alpha > 0
$$

Por tanto $g(\alpha) = 0$ para todo $\alpha > 0$. Luego:

$$
\boxed{X_{(1)} \text{ es completa para } \alpha \text{ cuando } \beta \text{ es conocido}}
$$

**¿**$\hat{\alpha}_{MM}$ es función de una estadística completa?

**NO**, porque:

1.  La estadística completa para $\alpha$ (con $\beta$ conocido) es
    $X_{(1)}$
2.  $\hat{\alpha}_{MM}$ depende de $\bar{X}$ y $S^2$
3.  $\hat{\alpha}_{MM}$ **no puede expresarse como función de**
    $X_{(1)}$ solamente

### f) Optimalidad

El estimador $\hat{\alpha}_{MM}$ obtenido por método de momentos para el
parámetro de escala $\alpha$ de la distribución Pareto **no es óptimo**
por tres razones fundamentales que se encadenan lógicamente. Primero, el
estimador **no es insesgado**, lo que significa que en promedio no
coincide con el valor verdadero del parámetro $\alpha$; esto ya lo
descalifica como candidato a óptimo bajo el marco clásico que busca
minimizar la varianza entre estimadores insesgados.

Segundo, y más importante, $\hat{\alpha}_{MM}$ **no utiliza toda la
información relevante** contenida en la muestra sobre el parámetro
$\alpha$: ignora completamente el valor mínimo muestral $X_{(1)}$, que
es la estadística suficiente y completa para $\alpha$ cuando el
parámetro de forma $\beta$ es conocido. Al basarse únicamente en la
media muestral $\bar{X}$ y la varianza muestral $S^2$, desperdicia la
pieza de información más crucial para estimar el límite inferior de la
distribución

Tercero, como consecuencia directa de lo anterior, **existe otro
estimador claramente superior**: el estimador
$\hat{\alpha}_{EIVUM} = \frac{n\beta-1}{n\beta} X_{(1)}$, que sí es
insesgado, está basado en la estadística suficiente y completa
$X_{(1)}$, y por el teorema de Lehmann-Scheffé posee varianza mínima
entre todos los estimadores insesgados. Por lo tanto, aunque
$\hat{\alpha}_{MM}$ es consistente (converge al valor verdadero cuando
el tamaño de muestra crece), no cumple las condiciones necesarias para
ser considerado óptimo en el sentido estadístico riguroso de tener
mínima varianza entre los estimadores insesgados.

## 1.4.2. Propiedades del Estimador $\hat{\beta}_{MM}$

### a) Insesgamiento:

Un estimador $\hat{\beta}$ es **insesgado** si su esperanza coincide con
el parámetro verdadero:

$$
E(\hat{\beta})=\beta.
$$

En nuestro caso, el estimador de método de momentos para $\beta$ es:

$$
\hat{\beta}_{MM}
=
1+\sqrt{1+\frac{\bar X^2}{S^2}},
$$

donde $\bar X$ es la media muestral y $S^2$ es la varianza muestral.

Como $\hat{\beta}_{MM}$ es una **función no lineal** de $(\bar X,S^2)$,
no se espera que cumpla en general que:

$$
E\left(1+\sqrt{1+\frac{\bar X^2}{S^2}}\right)=1+\sqrt{1+\frac{E(\bar X)^2}{E(S^2)}},
$$

por lo que **no es insesgado en muestras finitas**.\
Para verificarlo de manera empírica, estimamos el sesgo mediante
simulación Monte Carlo:

$$
\text{Sesgo}(\hat{\beta}_{MM}) = E(\hat{\beta}_{MM})-\beta.
$$

```{r, echo=FALSE, }
# ============================================================
# INSesgamiento: SESGO EMPÍRICO DEL ESTIMADOR MM PARA BETA
# ============================================================

set.seed(123)
B <- 3000
n <- length(reclamos)

betaMM_sim <- numeric(B)

for(b in 1:B){
  x <- alpha_true / (1 - runif(n))^(1/beta_true)
  xbar <- mean(x)
  s2   <- var(x)
  betaMM_sim[b] <- 1 + sqrt(1 + (xbar^2 / s2))
}

sesgo_betaMM <- mean(betaMM_sim) - beta_true
sesgo_betaMM

```

```{r}
library(ggplot2)

df_betaMM <- data.frame(betaMM = betaMM_sim)

ggplot(df_betaMM, aes(x = betaMM)) +
geom_density(fill = "skyblue", alpha = 0.6) +
geom_vline(xintercept = beta_true, linetype = "dashed", 
           color = "red", linewidth = 1) +
labs(title = "Insesgamiento del Estimador MM para β",
subtitle = "Si fuese insesgado, la distribución estaría centrada 
exactamente en β",
x = "Estimaciones de β (MM)",
y = "Densidad") +
theme_minimal()

```

**Conclusión:**

Dado que $E(\hat{\beta}{MM})\neq\beta$ (sesgo empírico distinto de 0),
se concluye que el estimador $\hat{\beta}{MM}$ no es insesgado en
muestras finitas.

### b) Consistencia:

Un estimador $\hat{\beta}_n$ es **consistente** para el parámetro
$\beta$ si converge en probabilidad al valor verdadero cuando el tamaño
muestral tiende a infinito, es decir,

$$
\hat{\beta}_n \xrightarrow{p} \beta \quad \text{cuando } n \to \infty.
$$

El estimador de método de momentos para el parámetro $\beta$ está dado
por:

$$
\hat{\beta}_{MM}
=
1+\sqrt{1+\frac{\bar X^2}{S^2}},
$$

donde $\bar X$ y $S^2$ representan la media y la varianza muestral,
respectivamente.

Para la distribución Pareto con $\beta>2$, se cumple que:

$$
\bar X \xrightarrow{p} E(X),
\qquad
S^2 \xrightarrow{p} \operatorname{Var}(X),
$$

por la Ley de los Grandes Números y la consistencia de la varianza
muestral cuando la varianza existe.

Definiendo la función:

$$
g(u,v)=1+\sqrt{1+\frac{u^2}{v}},
$$

la cual es continua para $v>0$, y dado que:

$$
\hat{\beta}_{MM}=g(\bar X,S^2),
$$

por el **Teorema de la Función Continua** se obtiene:

$$
\hat{\beta}_{MM}
\xrightarrow{p}
g\big(E(X),\operatorname{Var}(X)\big)
=
\beta.
$$

Por lo tanto, el estimador de método de momentos $\hat{\beta}_{MM}$ es
**consistente** para el parámetro $\beta$.

Para visualizar esta propiedad, se analiza el comportamiento del
estimador al aumentar el tamaño muestral.

```{r, echo=FALSE,warning=FALSE}
# ============================================================
# CONSISTENCIA: CONVERGENCIA DEL ESTIMADOR MM
# ============================================================

n_seq <- seq(20, 1000, by = 10)
betaMM_path <- numeric(length(n_seq))

for(i in seq_along(n_seq)){
  k <- n_seq[i]
  x_sub <- reclamos[1:k]
  betaMM_path[i] <- 1 + sqrt(1 + (mean(x_sub)^2 / var(x_sub)))
}

df_cons <- data.frame(n = n_seq, betaMM = betaMM_path)

library(ggplot2)
ggplot(df_cons, aes(x = n, y = betaMM)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = beta_true, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Consistencia del Estimador MM para β",
       subtitle = "Convergencia en probabilidad al valor verdadero al aumentar n",
       x = "Tamaño de muestra (n)",
       y = "Estimación de β") +
  theme_minimal()

```

**Conclusión:**

Al cumplirse que $\bar X \xrightarrow{p} E(X)$ y
$S^2 \xrightarrow{p} \operatorname{Var}(X)$, y dado que
$\hat{\beta}{MM}$ es una función continua de estos estadísticos, se
concluye que el estimador $\hat{\beta}{MM}$ es consistente para el
parámetro $\beta$.

### c) Suficiencia:

Un estadístico $T=T(X_1,\dots,X_n)$ es **suficiente** para un parámetro
$\beta$ si la distribución condicional de la muestra, dado $T$, no
depende de dicho parámetro.

El estimador de método de momentos para $\beta$ está dado por:

$$
\hat{\beta}_{MM}
=
1+\sqrt{1+\frac{\bar X^2}{S^2}},
$$

donde $\bar X$ y $S^2$ corresponden a la media y varianza muestral,
respectivamente.

En la distribución Pareto, cuando ambos parámetros $\alpha$ y $\beta$
son desconocidos, la estadística suficiente para $\beta$ está asociada a
la estructura de la función de verosimilitud y viene dada por funciones
del estadístico

$$
T = \sum_{i=1}^{n} \ln\left(\frac{X_i}{X_{(1)}}\right).
$$

Dado que el estimador $\hat{\beta}_{MM}$ depende únicamente de
$(\bar X,S^2)$ y no es función del estadístico suficiente $T$, se
concluye que **no se basa en una estadística suficiente** para el
parámetro $\beta$.

**Conclusión:**

El estimador $\hat{\beta}_{MM}$ **no es suficiente**.

### d) Ancilaridad

Un estadístico $T$ se denomina **ancilar** si su distribución no depende
del parámetro de interés. Es decir, $T$ es ancilar para $\beta$ si la
ley de probabilidad de $T$ es independiente de $\beta$.

El estimador de método de momentos para el parámetro $\beta$ está
definido como:

$$
\hat{\beta}_{MM}
=
1+\sqrt{1+\frac{\bar X^2}{S^2}}.
$$

Este estimador está construido con el objetivo explícito de aproximar el
valor del parámetro $\beta$. En consecuencia, su distribución muestral
depende del valor de dicho parámetro.

Por lo tanto, la distribución de $\hat{\beta}_{MM}$ **no es invariante
respecto a** $\beta$, lo que implica que el estimador no cumple la
propiedad de ancilaridad.

**Conclusión:**

El estimador $\hat{\beta}_{MM}$ **no es ancilar**.

### e) Completitud

La **completitud** es una propiedad que se define para **estadísticas
suficientes**.\
Un estadístico suficiente $T$ para un parámetro $\beta$ es completo si,
para toda función medible $g(\cdot)$, se cumple que:

$$
E_\beta[g(T)] = 0 \ \text{para todo } \beta
\quad \Longrightarrow \quad
P_\beta\big(g(T)=0\big)=1.
$$

Esta propiedad resulta fundamental en la teoría de la estimación óptima,
ya que permite garantizar la unicidad del estimador insesgado de
varianza mínima mediante el Teorema de Lehmann--Scheffé.

En el presente caso, el estimador de método de momentos para el
parámetro $\beta$ está dado por:

$$
\hat{\beta}_{MM}
=
1+\sqrt{1+\frac{\bar X^2}{S^2}},
$$

el cual depende de la media y la varianza muestral $(\bar X, S^2)$.\
Sin embargo, como se mostró en la sección anterior, estos estadísticos
**no constituyen una estadística suficiente** para el parámetro $\beta$
en la distribución Pareto.

Por lo tanto, **no corresponde analizar la propiedad de completitud**
para el estimador $\hat{\beta}_{MM}$, ya que dicha propiedad solo es
aplicable a estadísticas suficientes.

**Conclusión:**

La propiedad de completitud **no aplica** al estimador de método de
momentos $\hat{\beta}_{MM}$.

### f) Optimalidad:

Un estimador se considera **óptimo** si alcanza la mínima varianza
posible dentro de una clase determinada de estimadores, usualmente
dentro de la clase de estimadores insesgados.

El estimador de método de momentos para el parámetro $\beta$ se obtiene
igualando momentos teóricos y muestrales, sin hacer uso de la función de
verosimilitud ni de la estructura de la distribución muestral completa.
En consecuencia, el método de momentos **no garantiza eficiencia ni
mínima varianza**.

Además, como se mostró previamente, el estimador $\hat{\beta}_{MM}$ **no
es insesgado** en muestras finitas, lo que impide que pueda ser
considerado óptimo bajo criterios clásicos de optimalidad, como los
establecidos por el Teorema de Lehmann--Scheffé.

Por el contrario, el estimador EIVUM obtenido en la sección
correspondiente es insesgado, función de una estadística suficiente y
completa, y por tanto posee varianza uniformemente mínima dentro de su
clase.

**Conclusión:**

El estimador de método de momentos $\hat{\beta}_{MM}$ **no es óptimo**
para el parámetro $\beta$ de la distribución Pareto.

## 1.5 Estimador : Estimador Insesgado de Varianza Uniformemente Mínima (EIVUM) para el parámetro $\beta$

Dado que en la sección anterior se determinó que el Estimador de Máxima
Verosimilitud (EMV) para $\beta$ es sesgado, procederemos a obtener el
estimador óptimo aplicando el **Teorema de Lehmann-Scheffé**.

Según la teoría de la optimalidad, si logramos construir un estimador
insesgado que sea función de una estadística suficiente y completa,
dicho estimador será el **Estimador Insesgado de Varianza Uniformemente
Mínima (EIVUM)**.

El procedimiento se detalla en los siguientes cuatro pasos.

### Paso 1: Identificación de la Estadística Suficiente y Completa

Analizando la función de densidad de la distribución Pareto, observamos
que pertenece a la familia exponencial *k*-paramétrica respecto al
parámetro de forma $\beta$.

Bajo la condición de que el parámetro de escala $\alpha$ es desconocido
y estimado mediante el mínimo muestral $X_{(1)}$, la teoría de
suficiencia indica que la estadística $T$ que captura toda la
información sobre $\beta$ es la suma de los logaritmos de las razones
muestrales:

$$
T = \sum_{i=1}^{n} \ln\left( \frac{X_i}{X_{(1)}} \right)
$$

Esta estadística $T$ es **suficiente y completa** para el parámetro
$\beta$, condición necesaria para aplicar el teorema de Lehmann-Scheffé.

### Paso 2: Distribución Muestral de la Estadística

A partir de las propiedades de la transformación de variables aleatorias
Pareto, se deduce que la estadística $T$ sigue una distribución Gamma.

Dado que se ha estimado un parámetro adicional ($\alpha$), los grados de
libertad se ajustan a $(n-1)$. Por tanto:

$$
T \sim \text{Gamma}(n-1, \beta)
$$

### Paso 3: Verificación del Sesgo y Corrección (Método de la Esperanza)

Partimos del Estimador de Máxima Verosimilitud hallado previamente:

$$
\hat{\beta}_{EMV} = \frac{n}{T}
$$

Para verificar si es insesgado, calculamos su valor esperado
$E[\hat{\beta}_{EMV}]$. Utilizando la propiedad de la esperanza inversa
para una variable con distribución $\text{Gamma}(k, \lambda)$, donde:

$$
E\left[\frac{1}{T}\right] = \frac{\lambda}{k-1},
$$

se obtiene:

$$
E\left[\frac{1}{T}\right] = \frac{\beta}{(n-1)-1} = \frac{\beta}{n-2}
$$

Sustituyendo en la esperanza del EMV:

$$
E[\hat{\beta}_{EMV}] = n \cdot E\left[\frac{1}{T}\right]
= n \left( \frac{\beta}{n-2} \right)
= \left( \frac{n}{n-2} \right)\beta
$$

El resultado muestra que el EMV no es insesgado, ya que su valor
esperado no es exactamente $\beta$, sino que está escalado por el factor
$\frac{n}{n-2}$.

### Paso 4: Construcción del Estimador Óptimo (EIVUM)

Para eliminar el sesgo, aplicamos una corrección multiplicativa usando
el inverso del factor de sesgo encontrado $\left(\frac{n-2}{n}\right)$.

Definimos el nuevo estimador como:

$$
\hat{\beta}_{EIVUM} = \left( \frac{n-2}{n} \right) \cdot \hat{\beta}_{EMV}
$$

Reemplazando $\hat{\beta}_{EMV} = \frac{n}{T}$:

$$
\hat{\beta}_{EIVUM}
= \left( \frac{n-2}{n} \right) \cdot \frac{n}{T}
= \frac{n-2}{T}
$$

#### Conclusión

La fórmula final del estimador óptimo es:

$$
\boxed{
\hat{\beta}_{EIVUM}
= \frac{n-2}{\sum_{i=1}^{n} \ln\left( \frac{X_i}{X_{(1)}} \right)}
}
$$

Al haber corregido el sesgo, cumpliéndose que
$E[\hat{\beta}_{EIVUM}] = \beta$, y al depender únicamente de la
estadística suficiente y completa $T$, el **Teorema de Lehmann-Scheffé**
garantiza que este estimador es el de **menor varianza posible entre
todos los estimadores insesgados (UMVUE)** para el parámetro $\beta$ de
una distribución Pareto.

### 1.5.1 Verificación de Propiedades del Estimador ($\hat{\beta}_{EIVUM}$)

Recordamos la expresión del estimador y la distribución de la
estadística suficiente sobre la cual se basa:

$$
\hat{\beta}_{EIVUM} = \frac{n-2}{T}, \quad \text{donde } T \sim \text{Gamma}(n-1, \beta)
$$

#### a. Insesgabilidad

Un estimador es insesgado si su esperanza matemática coincide con el
parámetro a estimar. Calculamos:

$$
E[\hat{\beta}_{EIVUM}] = E\left[ \frac{n-2}{T} \right] = (n-2)\, E\left[\frac{1}{T}\right]
$$

Dado que para una variable $T \sim \text{Gamma}(k,\beta)$ se cumple
$E[1/T] = \frac{\beta}{k-1}$ con $k = n-1$, resulta:

$$
E[\hat{\beta}_{EIVUM}] = (n-2)\cdot \frac{\beta}{n-2} = \beta
$$

**Conclusión:** Se verifica que $E[\hat{\beta}_{EIVUM}] = \beta$, por lo
que el estimador es insesgado para todo $n > 2$.

```{r }
# Usamos la base de datos 'reclamos' generada en la sección 1.1
n_obs <- length(reclamos) # n = 1000
x_min_obs <- min(reclamos)

# Cálculo del estadístico T
T_obs <- sum(log(reclamos / x_min_obs))

# Cálculo del Estimador 5 (EIVUM)
beta_eivum_val <- (n_obs - 2) / T_obs

print(paste("Valor Verdadero Beta:", beta_true))
print(paste("Estimación EIVUM:", round(beta_eivum_val, 5)))
print(paste("Diferencia (Sesgo muestral):", round(beta_eivum_val - beta_true, 5)))
```

#### b. Eficiencia

La eficiencia se evalúa a través de la varianza del estimador.
Utilizando que para $T \sim \text{Gamma}(k,\beta)$, la varianza inversa
es $\text{Var}(1/T) = \frac{\beta^2}{(k-1)^2(k-2)}$ con $k = n-1$, se
obtiene:

$$
\text{Var}(\hat{\beta}_{EIVUM}) = (n-2)^2 \, \text{Var}\left(\frac{1}{T}\right)
$$

$$
\text{Var}(\hat{\beta}_{EIVUM}) = (n-2)^2 \cdot \frac{\beta^2}{(n-2)^2 (n-3)} = \frac{\beta^2}{n-3}
$$

**Conclusión:** La varianza es finita para $n > 3$. Además, dado que el
estimador es insesgado y depende únicamente de una estadística
suficiente y completa, el Teorema de Lehmann-Scheffé garantiza que esta
varianza es la mínima posible entre todos los estimadores insesgados,
confirmando que $\hat{\beta}_{EIVUM}$ es el estimador más eficiente de
su clase.

```{r , echo = F, fig.align='center',warning=FALSE}
# =============================================================================
# GRÁFICO: COMPARACIÓN DE EFICIENCIA (EIVUM vs EMV)
# =============================================================================
set.seed(123)
n_sim <- 50  # Usamos n pequeño para notar diferencias visuales
N_rep <- 5000 

sim_eivum <- numeric(N_rep)
sim_emv <- numeric(N_rep)

for(i in 1:N_rep){
  # Generar Pareto
  u <- runif(n_sim)
  x <- alpha_true / (1 - u)^(1/beta_true)
  
  T_val <- sum(log(x / min(x)))
  
  sim_eivum[i] <- (n_sim - 2) / T_val
  sim_emv[i] <- n_sim / T_val
}

df_eff <- data.frame(
  Valor = c(sim_eivum, sim_emv),
  Estimador = rep(c("EIVUM (Insesgado)", "EMV (Sesgado)"), each = N_rep)
)

library(ggplot2)
ggplot(df_eff, aes(x = Valor, fill = Estimador)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = beta_true, linetype = "dashed", size = 1) +
  scale_fill_manual(values = c("green3", "tomato")) +
  labs(title = "Comparación de Eficiencia: EIVUM vs EMV",
       subtitle = "El EIVUM (Verde) está centrado en 3. El EMV (Rojo) está desplazado a la derecha.",
       x = "Estimación de Beta", y = "Densidad") +
  theme_minimal() +
  xlim(1.5, 5)
```

#### c. Consistencia

Un estimador es consistente si converge en probabilidad al parámetro
verdadero cuando el tamaño muestral tiende a infinito. Una condición
suficiente es:

1.  $E[\hat{\beta}_{EIVUM}] \to \beta$ cuando $n \to \infty$
2.  $\text{Var}(\hat{\beta}_{EIVUM}) \to 0$ cuando $n \to \infty$

Verificación:

-   Condición 1: $$
    \lim_{n \to \infty} E[\hat{\beta}_{EIVUM}] = \beta
    $$

-   Condición 2: $$
    \lim_{n \to \infty} \text{Var}(\hat{\beta}_{EIVUM}) = \lim_{n \to \infty} \frac{\beta^2}{n-3} = 0
    $$

**Conclusión:** Al cumplirse ambas condiciones, el estimador
$\hat{\beta}_{EIVUM}$ converge en probabilidad al parámetro $\beta$, por
lo que es un estimador consistente.

```{r , echo = F, fig.align='center',warning=FALSE}
# =============================================================================
# GRÁFICO: CONSISTENCIA DEL EIVUM (CONVERGENCIA)
# =============================================================================
# Usamos la base grande 'reclamos' (n=1000)
n_seq <- seq(10, 1000, by = 5)
est_trayectoria <- numeric(length(n_seq))

for(i in 1:length(n_seq)){
  k <- n_seq[i]
  sub_x <- reclamos[1:k]
  T_sub <- sum(log(sub_x / min(sub_x)))
  est_trayectoria[i] <- (k - 2) / T_sub
}

df_cons <- data.frame(n = n_seq, Estimacion = est_trayectoria)

ggplot(df_cons, aes(x = n, y = Estimacion)) +
  geom_line(color = "blue", size = 0.8) +
  geom_hline(yintercept = beta_true, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Consistencia del Estimador 5 (EIVUM)",
       subtitle = "Convergencia al valor verdadero (3) al aumentar la muestra",
       x = "Tamaño de muestra (n)", y = "Valor Estimado") +
  theme_minimal() +
  ylim(2.5, 3.5)
```

#### d. Suficiencia

La propiedad de suficiencia indica que el estimador utiliza toda la
información disponible en la muestra sobre el parámetro, sin
desperdiciar datos.

**Conclusión:** Como se demostró en el **Paso 1** de la construcción del
estimador, $\hat{\beta}_{EIVUM}$ es una función inyectiva de la
estadística $T = \sum \ln(X_i/X_{(1)})$. Dado que se probó que $T$ es
una estadística suficiente para $\beta$ (por pertenecer a la Familia
Exponencial), el estimador hereda esta propiedad. Por tanto, es un
estimador **suficiente**.

#### e. Ancilaridad

Según la definición teórica, una estadística es ancilar si su
distribución no depende del parámetro de interés.

En este caso, el estimador construido es
$\hat{\beta}_{EIVUM} = \frac{n-2}{T}$. Para evaluar si el estimador es
ancilar, analizamos sus momentos:

$$
E[\hat{\beta}_{EIVUM}] = \beta, \qquad \text{Var}(\hat{\beta}_{EIVUM}) = \frac{\beta^2}{n-3}
$$

Dado que la esperanza y la varianza dependen explícitamente de $\beta$,
la distribución del estimador cambia según el valor del parámetro.

**Conclusión:** El estimador $\hat{\beta}_{EIVUM}$ **no es una
estadística ancilar**, ya que su distribución depende del parámetro
$\beta$. Este comportamiento es esperado y adecuado para un estimador
puntual del parámetro de interés.

#### f. Completitud

La propiedad de completitud es fundamental para garantizar la unicidad
del estimador óptimo según el Teorema de Lehmann-Scheffé.

La estadística suficiente es
$T = \sum_{i=1}^{n} \ln\left(\frac{X_i}{X_{(1)}}\right)$. Se ha
demostrado previamente que $T \sim \text{Gamma}(n-1, \beta)$. La
distribución Gamma con uno de sus parámetros desconocidos pertenece a la
familia exponencial regular de un parámetro. Dado que el espacio
paramétrico es abierto ($\beta > 0$), se cumple que la estadística
suficiente $T$ es completa.

Esto implica que si una función medible $g(T)$ satisface $E[g(T)] = 0$
para todo $\beta$, entonces $P(g(T) = 0) = 1$.

**Conclusión:** La estadística $T$ es una estadística suficiente y
completa para el parámetro $\beta$.

#### g. Optimalidad (Conclusión Final)

Se ha verificado que el estimador $\hat{\beta}_{EIVUM} = \frac{n-2}{T}$
cumple las siguientes propiedades:

-   Es insesgado: $E[\hat{\beta}_{EIVUM}] = \beta$.
-   Es función de una estadística suficiente y completa ($T$).
-   Posee varianza mínima dentro de la clase de estimadores insesgados.

Por lo tanto, aplicando el **Teorema de Lehmann-Scheffé**, se concluye
que:

> Un estimador insesgado que sea función de una estadística suficiente y
> completa es el Estimador Insesgado de Varianza Uniformemente Mínima.

**Conclusión General:** $\hat{\beta}_{EIVUM}$ es el **Estimador
Insesgado de Varianza Uniformemente Mínima (EIVUM)** para el parámetro
$\beta$ de la distribución Pareto. No existe otro estimador insesgado
con menor varianza que este.

# 2) Pregunta 2 - Intervalos de confianza

## 2.1 Estimador 1 (el mínimo para alfa)

Vamos a hallar un intervalo de confianza del 90%:

$$
\begin{aligned}
\hat{\alpha}_{EMV} &= y_1 = \min(x_1, \ldots, x_n); \\
f_Y(y) &= \frac{n\beta \alpha^{n\beta}}{y^{n\beta+1}}, \quad y > \alpha
\end{aligned}
$$

Un posible pivote es $W = Y_1 / \alpha$, porque su distribución no
debería depender de $\alpha$. Hacemos un cambio de variable
$Q = W = \frac{Y_1}{\alpha}$. Con el jacobiano $dy_1 = \alpha \, dw$, se
tiene:

$$
f_W(w) = f_{Y_1}(\alpha w) \cdot \alpha
= n\beta \, \alpha^{n\beta} (\alpha w)^{-(n\beta+1)} \cdot \alpha
$$ 
$$
= n\beta \, \alpha^{n\beta} \alpha^{-n\beta-1} w^{-(n\beta+1)} \alpha
$$ 
$$
= n\beta \, w^{-(n\beta+1)}, \quad w > 1.
$$

La densidad resultante \text{no depende de $\alpha$}, por lo tanto $W$
es un \text{pivote}.

Hallamos la función de distribución de W:

$$
\begin{aligned}
F_W(w) &= \int_1^w n\beta t^{-(n\beta+1)} \, dt \\
&= n\beta \left[ \frac{t^{-n\beta}}{-n\beta} \right]_1^w \\
&= n\beta \left[ -\frac{t^{-n\beta}}{n\beta} \right]_1^w \\
&= 1 - w^{-n\beta}, \quad w > 1.
\end{aligned}
$$

Usando el método de pivote: $P(a \le W \le b) = 0.90$. Un posible
intervalo sería el siguiente:

$$
\begin{aligned}
P(w \leq a) = F_W(a) &= 1 - a^{-n\beta} = 0.05 \quad \rightarrow \quad a = (0.95)^{-1/(n\beta)} \\
P(w \leq b) = F_W(b) &= 1 - b^{-n\beta} = 0.95 \quad \rightarrow \quad b = (0.05)^{-1/(n\beta)}
\end{aligned}
$$\
Entonces:

$$
\begin{array}{r}
P\left[a \leq \frac{Y_1}{\alpha} \leq b\right]=0.90 \rightarrow P\left[\frac{Y_1}{b} \leq \alpha \leq \frac{Y_1}{a}\right]=P\left[\frac{Y_1}{(0.6)^{-1 / n \beta}} \leqslant \alpha \leqslant \frac{Y_1}{(0.95)^{-1 / n \beta}}\right] \\ \\
\text { I.C }=\left[Y_1 \cdot(0.05)^{1 /(n \beta)} ; Y_1 \cdot(0.95)^{1 /(n \beta)}\right]
\end{array}
$$
## 2.2 Estimador 2: Suma Log de T

Sea $X_1, \ldots, X_n$ una muestra aleatoria i.i.d. de una distribución
Pareto$(\alpha,\beta)$ con densidad

$$
f(x)=\frac{\beta \alpha^\beta}{x^{\beta+1}} I_{(\alpha,\infty)}(x).
$$

El estimador de máxima verosimilitud de $\beta$ es

$$
\hat{\beta}_{EMV}
= \frac{n}{\sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right)}.
$$

Definimos la variable aleatoria

$$
T = \sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right).
$$

Sabemos que, si $X \sim \text{Pareto}(\alpha,\beta)$, entonces
$\ln(X/\alpha) \sim \text{Exponencial}(\beta)$.
Por lo tanto,

$$
T \sim \text{Gamma}(n,\beta).
$$

Un pivote adecuado es

$$
W = 2\beta T,
$$

ya que

$$
W \sim \chi^2_{2n}.
$$

Para un nivel de confianza del $90\%$ se tiene $\alpha = 0.10$, por lo que

$$
P\left(
\chi^2_{2n,0.05}
\le 2\beta T
\le \chi^2_{2n,0.95}
\right) = 0.90.
$$

Despejando $\beta$:

$$
P\left(
\frac{\chi^2_{2n,0.05}}{2T}
\le \beta
\le
\frac{\chi^2_{2n,0.95}}{2T}
\right) = 0.90.
$$

Por lo tanto, el intervalo de confianza al $90\%$ para $\beta$ es

$$
\boxed{
\text{I.C.}_{90\%}(\beta)=
\left[
\frac{\chi^2_{2n,0.05}}{2\sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right)}
\; ; \;
\frac{\chi^2_{2n,0.95}}{2\sum_{i=1}^n \ln\left(\frac{X_i}{\alpha}\right)}
\right]
}
$$

En la práctica, si $\alpha$ es desconocido, se reemplaza por
$\hat{\alpha}_{EMV} = \min(X_1,\ldots,X_n)$.

### 2.3.1 Aplicación 
```{r include=FALSE}
# tamaño de muestra
n <- length(reclamos)

# alpha EMV
alpha_hat <- min(reclamos)

# estadístico T
T <- sum(log(reclamos / alpha_hat))

T
```
Sea
$$
T = \sum_{i=1}^n \ln\left(\frac{X_i}{\hat{\alpha}_{EMV}}\right).
$$

Bajo el supuesto de una distribución Pareto$(\alpha,\beta)$ se tiene que:
$$
2\beta T \sim \chi^2_{2n}.
$$

Entonces, un intervalo de confianza del 90\% para el parámetro $\beta$ está dado por:
$$
\text{IC}_{90\%}(\beta)=
\left[
\frac{\chi^2_{2n,\,0.05}}{2T}
\; ; \;
\frac{\chi^2_{2n,\,0.95}}{2T}
\right].
$$

Para los datos observados se obtuvo:
$$
T = 328.563,
$$
por lo que el intervalo queda completamente determinado al sustituir dicho valor.

```{r include=FALSE}
# ===============================
# Intervalo de Confianza 90% para beta (Pareto)
# ===============================

T <- 328.563
n <- length(reclamos)

alpha_ic <- 0.10

li <- qchisq(alpha_ic/2, df = 2*n) / (2*T)
ls <- qchisq(1 - alpha_ic/2, df = 2*n) / (2*T)

c(li, ls)

```

```{r echo=FALSE}
paste0(
  "IC 90% para beta: [",
  round(li, 4), ", ",
  round(ls, 4), "]"
)

```
Esto significa que, bajo el supuesto de que la muestra proviene de una
distribución Pareto$(\alpha,\beta)$, el intervalo contiene al verdadero valor del parámetro $\beta$ con una probabilidad del 90\%.

Asimismo, se observa que el valor verdadero del parámetro
($\beta = 3$) pertenece al intervalo estimado, lo cual es consistente con las propiedades teóricas del estimador de máxima verosimilitud y respalda la adecuación del modelo ajustado.

Finalmente, el ancho relativamente reducido del intervalo se explica por
el tamaño muestral grande ($n = 1000$), lo que refleja una mayor precisión en la estimación del parámetro.

## 2.3 Estimador 3: Media Muestral ($\bar{X}$) para la media poblacional $\mu$

Para la construcción del intervalo de confianza de la media poblacional
$\mu = E(X)$, utilizaremos el **Método Asintótico** basado en el Teorema
del Límite Central (TLC).

### 2.3.1 Justificación Teórica

Dado que el tamaño de muestra es suficientemente grande ($n = 1000$) y
que en nuestro modelo la varianza poblacional existe (puesto que
$\beta = 3 > 2$), el estadístico media muestral $\bar{X}$ sigue una
distribución aproximadamente normal:

$$ \bar{X} \xrightarrow{d} N\left( \mu, \frac{\sigma^2}{n} \right) $$

Como la varianza poblacional $\sigma^2$ es desconocida, utilizamos la
varianza muestral $S^2$ como estimador consistente, definiendo la
siguiente **Cantidad Pivotal Asintótica**:

$$ Z = \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim N(0, 1) $$

### 2.3.2 Construcción del Intervalo

Para un nivel de confianza del $(1 - \gamma)$, buscamos los valores
críticos de la distribución normal estándar tales que:

$$ P\left( -z_{1-\gamma/2} \leq \frac{\bar{X} - \mu}{S / \sqrt{n}} \leq z_{1-\gamma/2} \right) = 1 - \gamma $$

Despejando el parámetro $\mu$, obtenemos la expresión del intervalo de
confianza:

$$ 
IC(\mu)_{1-\gamma} = \left[ \bar{X} - z_{1-\gamma/2} \frac{S}{\sqrt{n}} \quad ; \quad \bar{X} + z_{1-\gamma/2} \frac{S}{\sqrt{n}} \right] 
$$

### 2.3.3 Aplicación en R

Utilizamos los datos de la variable `reclamos` para calcular el
intervalo al 95% de confianza.

```{r intervalo_mu, echo=TRUE}
# ==============================================================================
# CÁLCULO DEL INTERVALO DE CONFIANZA ASINTÓTICO PARA LA MEDIA (MU)
# ==============================================================================

# 1. Parámetros de la muestra
n <- length(reclamos)
media_muestral <- mean(reclamos)
desv_estandar <- sd(reclamos)
confianza <- 0.95
gamma <- 1 - confianza

# 2. Valor crítico Z
z_critico <- qnorm(1 - gamma/2)

# 3. Error estándar y márgenes
error_estandar <- desv_estandar / sqrt(n)
margen_error <- z_critico * error_estandar

# 4. Límites del intervalo
lim_inf_mu <- media_muestral - margen_error
lim_sup_mu <- media_muestral + margen_error

# 5. Valor teórico para comparación
mu_teorico <- (alpha_true * beta_true) / (beta_true - 1)

# Reporte
cat(
  "INTERVALO DE CONFIANZA PARA LA MEDIA (TLC)\n\n",
  "Media Muestral (X_bar): ", round(media_muestral, 5), "\n",
  "Desviación Estándar (S): ", round(desv_estandar, 4), "\n",
  "Nivel de Confianza: 95%\n\n",
  "Límite Inferior: ", round(lim_inf_mu, 5), "\n",
  "Límite Superior: ", round(lim_sup_mu, 5), "\n",
  "Valor Teórico (mu): ", mu_teorico, "\n"
)
```

### 2.3.4 Interpretación

Con un nivel de confianza del 95%, el valor esperado de los reclamos se
encuentra entre `r round(lim_inf_mu, 4)` y `r round(lim_sup_mu, 4)`.
Observamos que el **valor teórico (**$\mu = 3$) se encuentra contenido
dentro del intervalo, lo que valida la precisión del estimador $\bar{X}$
y la eficacia de la aproximación por el Teorema del Límite Central para
este tamaño de muestra.

## 2.5 Estimador 5 (para $\beta$ con el EIVUM)

Para la construcción del intervalo de confianza para el parámetro de
forma $\beta$, usaremos el **Método de la Cantidad Pivotal**.

### 2.5.1 Deducción de la Cantidad Pivotal

Partiendo de los resultados obtenidos en la sección de estimación
puntual, la estadística suficiente para $\beta$ se define como:

$$
T = \sum_{i=1}^{n} \ln\left( \frac{X_i}{X_{(1)}} \right)
$$

Se conoce que esta estadística sigue una distribución Gamma con
parámetros de forma $n-1$ y tasa $\beta$:

$$
T \sim \text{Gamma}(n-1, \beta)
$$

A partir de las propiedades de la distribución Gamma, es posible
construir una función de las variables muestrales y del parámetro que
siga una distribución conocida e independiente de $\beta$. Al aplicar la
transformación $2\beta T$, obtenemos:

$$
Q = 2\beta T \sim \chi^2_{(2(n-1))}
$$

La variable $Q$ constituye una **cantidad pivotal válida**, dado que su
distribución es una Chi-cuadrado con $2n-2$ grados de libertad, la cual
no depende de los parámetros desconocidos de la población.

### 2.5.2 Construcción del Intervalo

Para obtener un intervalo con un nivel de confianza del $(1 - \alpha)$,
se plantea la siguiente ecuación de probabilidad:

$$
P\left( \chi^2_{\alpha/2} < 2\beta T < \chi^2_{1-\alpha/2} \right) = 1 - \alpha
$$

Donde $\chi^2_{p}$ denota el cuantil de probabilidad acumulada $p$ de la
distribución Chi-cuadrado con $gl = 2(n-1)$.

Al despejar algebraicamente el parámetro $\beta$ de la desigualdad, se
obtiene:

$$
\frac{\chi^2_{\alpha/2}}{2T} < \beta < \frac{\chi^2_{1-\alpha/2}}{2T}
$$

En consecuencia, el intervalo de confianza de $(1-\alpha)\%$ para el
parámetro $\beta$ queda definido por:

$$
IC(\beta) = \left[ \frac{\chi^2_{\alpha/2, \, 2(n-1)}}{2T} \, ; \, \frac{\chi^2_{1-\alpha/2, \, 2(n-1)}}{2T} \right]
$$

### 2.5.3 Aplicación

A continuación, aplicamos este método a nuestra base de datos `reclamos`
generada en la simulación ($n=1000$, $\alpha=2$, $\beta=3$).

```{r intervalo_confianza_pareto, echo=TRUE}
# ==============================================================================
# CÁLCULO DEL INTERVALO DE CONFIANZA EXACTO PARA BETA
# Método del Pivote (Chi-Cuadrado)
# ==============================================================================

# 1. Configuración
# Usamos la variable 'reclamos' que ya existe en el entorno
datos_analisis <- reclamos 
n <- length(datos_analisis)
nivel_confianza <- 0.95
alfa <- 1 - nivel_confianza

# 2. Cálculo de la Estadística Suficiente (T)
x_min <- min(datos_analisis)
T_stat <- sum(log(datos_analisis / x_min))

# 3. Valores Críticos de la Chi-Cuadrado
# El pivote es Q = 2*beta*T ~ Chi^2(gl = 2(n-1))
gl <- 2 * (n - 1)

chi_inf <- qchisq(alfa / 2, df = gl)      # Cola izquierda
chi_sup <- qchisq(1 - alfa / 2, df = gl)  # Cola derecha

# 4. Construcción del Intervalo
# Fórmula: [ Chi_Inf / 2T  ;  Chi_Sup / 2T ]
Limite_Inferior <- chi_inf / (2 * T_stat)
Limite_Superior <- chi_sup / (2 * T_stat)

# 5. Reporte de Resultados
cat(
  "INTERVALO DE CONFIANZA EXACTO PARA BETA\n\n",
  "Tamaño de muestra (n): ", n, "\n",
  "Estadística T: ", round(T_stat, 4), "\n",
  "Grados de Libertad: ", gl, "\n\n",
  "Límite Inferior: ", round(Limite_Inferior, 5), "\n",
  "Límite Superior: ", round(Limite_Superior, 5), "\n"
)
```

### 2.5.4 Interpretación de Resultados

Con un nivel de confianza del 95%, estimamos que el verdadero valor del
parámetro de forma $\beta$ se encuentra dentro del intervalo:

$$
IC(\beta)_{95\%} = [2.85487, \, 3.23192]
$$

**Análisis de la simulación:**

Dado que en nuestro diseño experimental el valor verdadero del parámetro
es $\beta = 3$, podemos confirmar que el intervalo calculado ha
**capturado exitosamente** al parámetro.

Adicionalmente, observamos que la longitud del intervalo es reducida
($L \approx 0.377$), lo cual evidencia una **alta precisión** en la
estimación. Esto es consecuencia directa de dos factores:

1.  El uso de un estimador basado en una estadística suficiente (EIVUM),
    que minimiza la varianza.
2.  El tamaño de muestra grande ($n=1000$), que reduce el error
    estándar.

# 4) Pregunta 4 - Pruebas de hipótesis para los estimadores

## 4.3 Prueba de Hipótesis para la Media Poblacional ($\mu$)

En esta sección, evaluamos la validez de una afirmación sobre el valor
central de la población mediante la media muestral $\bar{X}$. A
diferencia de los estimadores de los parámetros de forma ($\beta$), la
media muestral nos permite realizar inferencia sobre el valor esperado
de la variable "reclamos".

### 4.3.1 Fundamento Asintótico

Para la construcción de esta prueba, no nos basamos en una distribución
exacta (como la Gamma), sino en el **Teorema del Límite Central (TLC)**.
Dado que nuestra muestra $n=1000$ es grande y hemos verificado que
$\beta > 2$ (lo que garantiza la existencia de una varianza finita), se
cumple que:

1.  Por TLC: $\bar{X} \sim N(\mu, \sigma^2/n)$ asintóticamente.
2.  Por Consistencia: $S^2 \xrightarrow{P} \sigma^2$.
3.  Por **Teorema de Slutsky**: El estadístico pivot
    $Z = \frac{\bar{X} - \mu}{S/\sqrt{n}}$ converge en distribución a
    una Normal Estándar $N(0,1)$.

### 4.3.2 Planteamiento Formal de la Prueba

Definimos una prueba de hipótesis de dos colas para verificar si la
media poblacional es consistente con el valor teórico esperado bajo los
parámetros originales ($\mu_0 = 3$).

$$
\begin{aligned}
H_0: \mu = 3 \quad &\text{(Hipótesis Nula)} \\
H_1: \mu \neq 3 \quad &\text{(Hipótesis Alternativa)}
\end{aligned}
$$

**Nivel de Significancia (**$\alpha$): 0.05.

**Regla de Decisión:** Se rechaza $H_0$ si
$|Z_{calc}| > Z_{1-\alpha/2}$, donde $Z_{0.975} = 1.96$.
Alternativamente, se rechaza si el $p-valor < 0.05$.

### 4.3.3 Ejecución y Visualización en R

Para darle mayor peso al análisis, incluiremos una gráfica de la
densidad normal que muestre la ubicación de nuestro estadístico respecto
a la zona de rechazo.

```{r prueba_d_robusta, echo=TRUE, fig.height=5,warning=FALSE}
# 1. Configuración de parámetros
mu_0 <- 3
n <- length(reclamos)
x_bar <- mean(reclamos)
s_dev <- sd(reclamos)

# 2. Cálculo del estadístico y P-valor
z_calc <- (x_bar - mu_0) / (s_dev / sqrt(n))
p_val <- 2 * pnorm(-abs(z_calc))

# 3. Visualización de la Región de Rechazo
x_vals <- seq(-4, 4, length = 200)
y_vals <- dnorm(x_vals)
df_plot <- data.frame(x = x_vals, y = y_vals)

library(ggplot2)
ggplot(df_plot, aes(x = x, y = y)) +
  geom_line(color = "black", linewidth = 1) +
  # Sombreado de regiones de rechazo
  geom_area(data = subset(df_plot, x <= -1.96), fill = "red", alpha = 0.5) +
  geom_area(data = subset(df_plot, x >= 1.96), fill = "red", alpha = 0.5) +
  # Línea del estadístico calculado
  geom_vline(xintercept = z_calc, color = "blue", linetype = "dashed", linewidth = 1) +
  annotate("text", x = z_calc, y = 0.2, label = paste("Z_calc =", round(z_calc, 3)), 
           angle = 90, vjust = -0.5, color = "blue") +
  labs(title = "Regiones de Rechazo y Estadístico Z para la Media",
       subtitle = paste("P-valor =", round(p_val, 4)),
       x = "Z", y = "Densidad") +
  theme_minimal()
```

### 4.3.4 Análisis de Resultados y Dualidad

1.  **Estadístico Observado:** El valor de
    $Z_{calc} = `r round(z_calc, 4)`$ cae dentro de la región de **no
    rechazo** (área blanca entre -1.96 y 1.96).
2.  **Conclusión Estadística:** Dado que el
    $p-valor = `r round(p_val, 4)` > 0.05$, no existe evidencia
    suficiente para rechazar la hipótesis nula. Se concluye que la media
    de la muestra de reclamos es estadísticamente igual a 3.
3.  **Diferencia con los estimadores de Beta:** A diferencia de las
    pruebas de hipótesis realizadas para los estimadores EIVUM o EMV,
    que se basan en la distribución Gamma o Chi-cuadrado, esta prueba
    utiliza la **Normalidad Asintótica**. Esto es válido únicamente por
    el gran tamaño de la muestra ($n=1000$).
4.  **Relación con el Intervalo de Confianza:** Esta conclusión refuerza
    lo hallado en el punto 2.3. Se cumple la propiedad de **Dualidad de
    la Inferencia**: un valor de hipótesis nula $\mu_0$ no se rechaza al
    nivel $\alpha$ si y solo si dicho valor está contenido en el
    intervalo de confianza del $(1-\alpha)100\%$.

## 4.5 Prueba de Hipótesis para $\beta$ con el EIVUM

Utilizando el estimador EIVUM y la cantidad pivotal deducida
anteriormente, procederemos a verificar una hipótesis sobre el parámetro
de forma $\beta$.

### 4.5.1 Planteamiento de la Hipótesis

Dado que en nuestra simulación el valor verdadero es 3, plantearemos una
prueba bilateral para verificar si la evidencia muestral soporta este
valor:

$$
\begin{cases}
H_0: \beta = 3 \\
H_1: \beta \neq 3
\end{cases}
$$

### 4.5.2 Estadístico de Prueba

Bajo el supuesto de que la Hipótesis Nula ($H_0$) es cierta
($\beta = \beta_0$), utilizamos la cantidad pivotal $Q$ como nuestro
estadístico de prueba.

$$
Q_{cal} = 2\beta_0 T = 2\beta_0 \sum_{i=1}^{n} \ln\left( \frac{X_i}{X_{(1)}} \right)
$$

Sabemos que bajo $H_0$: $$
Q_{cal} \sim \chi^2_{(2(n-1))}
$$

### 4.5.3 Regla de Decisión

Para un nivel de significancia $\alpha = 0.05$, rechazaremos $H_0$ si el
estadístico calculado cae en la región de rechazo (colas de la
distribución):

-   **Rechazar** $H_0$ si: $Q_{cal} < \chi^2_{\alpha/2}$ ó
    $Q_{cal} > \chi^2_{1-\alpha/2}$
-   **No Rechazar** $H_0$ si:
    $\chi^2_{\alpha/2} \le Q_{cal} \le \chi^2_{1-\alpha/2}$

Donde los grados de libertad son $gl = 2(n-1)$.

### 4.5.4 Aplicación a la Base de Datos Simulada

```{r prueba_hipotesis_beta, echo=TRUE}
# ==============================================================================
# PRUEBA DE HIPÓTESIS EXACTA PARA BETA (MÉTODO DEL PIVOTE)
# ==============================================================================

# 1. Datos y Parámetros
datos_ph <- reclamos
n <- length(datos_ph)
beta_hipotesis <- 3  # Valor bajo H0
alfa <- 0.05

# 2. Cálculo del Estadístico de Prueba
# a) Estadística Suficiente T
x_min <- min(datos_ph)
T_stat <- sum(log(datos_ph / x_min))

# b) Estadístico Pivotal Q (Bajo H0)
Q_cal <- 2 * beta_hipotesis * T_stat

# 3. Valores Críticos (Chi-Cuadrado)
gl <- 2 * (n - 1)
chi_inf <- qchisq(alfa / 2, df = gl)
chi_sup <- qchisq(1 - alfa / 2, df = gl)

# 4. Cálculo del P-valor (Bilateral)
# p = 2 * min(P(X < Q), P(X > Q))
p_val_lower <- pchisq(Q_cal, df = gl)
p_val_upper <- 1 - pchisq(Q_cal, df = gl)
p_valor <- 2 * min(p_val_lower, p_val_upper)

# Decisión Automática
decision <- ifelse(p_valor < alfa, "Rechazar H0", "No Rechazar H0")

# 5. Reporte de Resultados (Estilo limpio)
cat(
  "PRUEBA DE HIPÓTESIS PARA BETA (H0: B=3)\n\n",
  "Estadístico T: ", round(T_stat, 4), "\n",
  "Estadístico Q (Calc): ", round(Q_cal, 4), "\n",
  "Grados de Libertad: ", gl, "\n\n",
  "Valor Crítico Inf: ", round(chi_inf, 4), "\n",
  "Valor Crítico Sup: ", round(chi_sup, 4), "\n\n",
  "P-valor: ", format(p_valor, scientific = FALSE), "\n",
  "Decisión al 5%: ", decision, "\n"
)

```

```{r , echo = F, fig.align='center',warning=FALSE}
# --- GRÁFICO DE LA PRUEBA ---
library(ggplot2)

# Crear datos para la curva Chi-cuadrado
x_vals <- seq(min(chi_inf) - 100, max(chi_sup) + 100, length.out = 1000)
y_vals <- dchisq(x_vals, df = gl)
df_plot <- data.frame(x = x_vals, y = y_vals)

# Graficar
ggplot(df_plot, aes(x = x, y = y)) +
  # Curva de densidad
  geom_line(color = "black", linewidth = 1) +
  
  # Áreas de rechazo (Colas rojas)
  geom_area(data = subset(df_plot, x < chi_inf), fill = "tomato", alpha = 0.5) +
  geom_area(data = subset(df_plot, x > chi_sup), fill = "tomato", alpha = 0.5) +
  
  # Área de no rechazo (Centro verde)
  geom_area(data = subset(df_plot, x >= chi_inf & x <= chi_sup), fill = "lightgreen", alpha = 0.3) +
  
  # Líneas verticales
  geom_vline(xintercept = c(chi_inf, chi_sup), linetype = "dashed", color = "red") +
  geom_vline(xintercept = Q_cal, color = "blue", linewidth = 1.2) +
  
  # Textos y etiquetas
  labs(title = "Prueba de Hipótesis Bilateral para Beta",
       subtitle = paste("El estadístico calculado Q =", round(Q_cal, 2), "cae en la zona de No Rechazo"),
       x = "Estadístico Chi-Cuadrado", y = "Densidad") +
  
  annotate("text", x = Q_cal, y = max(y_vals)*0.2, label = "Q_cal", color = "blue", vjust = -1) +
  annotate("text", x = chi_inf, y = max(y_vals)*0.5, label = "Lím. Inf", color = "red", angle = 90, vjust = 1.5) +
  annotate("text", x = chi_sup, y = max(y_vals)*0.5, label = "Lím. Sup", color = "red", angle = 90, vjust = -1) +
  theme_minimal()
```

### 4.5.5 Interpretación de la Prueba

Basándonos en los resultados obtenidos con la muestra simulada
($n=1000$):

1.  **Evidencia Estadística:** El estadístico de prueba calculado
    ($Q_{cal} \approx 1971.38$) se encuentra dentro de la región de
    aceptación definida por los valores críticos
    $[1876.01, \, 2123.78]$.
2.  **P-valor:** El P-valor obtenido ($0.68$) es muy superior al nivel
    de significancia ($\alpha = 0.05$).

**Conclusión:**

No existe evidencia suficiente para rechazar la hipótesis nula
$H_0: \beta = 3$. Esto confirma que el estimador EIVUM y los datos
generados son consistentes con el verdadero parámetro poblacional
utilizado en la simulación. Gráficamente, observamos claramente cómo la
línea azul (nuestro estadístico) cae cómodamente en la zona verde (zona
de no rechazo).
